{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this notebook, we introduce how to query the anotation database and give a flavor of how to use the results to do some simple analysis. By the end, you should be able to query for neurons, query for synapses between those neurons, and visualize the neurons and synapes.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Background</h2>\n",
    "\n",
    "\n",
    "This dataset is part of a collaboration between the Lee Lab at Harvard University and the Tuthill Lab at the University of Washington. Before beginning on this tutorial, please make sure you have completed the onboarding instructions which can be found [here](https://docs.google.com/document/d/10fnQHWTPluKeNXko6C63iKTKVwqXeSEQ10ftYgtEfhc/edit)\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Local setup instructions</h2>\n",
    "\n",
    "<p>\n",
    "To query and visualize the data, you will need a few custom packages developed by the Allen Institute and Princeton University.  If you want to setup your local computer's python system to use these packages you'll need to install them and their dependancies. \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The easiest way to install is to use Anaconda python.  Open up a terminal, or a command prompt on windows.  Navigate to the directory where you have checked out the swdb_2021 repository. Activate a conda environment if you'd like then type..\n",
    "\n",
    "</div>\n",
    "<h4>Linux/OSX</h4>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "./scripts/em_conda_install.sh\n",
    "</div>\n",
    "<h4>Windows</h4>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "./scripts/em_conda_install.bat\n",
    "</div>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "You'll need to restart your kernel after you do the install\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3D Visualization Methods </h2>\n",
    "\n",
    "EM data involves high resolution reconstructions of neurons, and so visualizing them interatively in 3D is essential to understanding the data (plus it's just fun).  This requires use of more specialized plotting packages than matplotlib.  There are a few options that have different plusses and minuses that are summarized below.  We'll show you code snippets from all methods, but execute the one based upon the viz_method variable defined in the next cell.\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><h3>viz_method</h3></td>\n",
    "        <td><h3>pros</h3></td>\n",
    "        <td><h3>cons</h3></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "           vtk\n",
    "        </td>\n",
    "        <td>Fastest <br/>\n",
    "            Most features: <br/>\n",
    "            mesh coloring<br/>  \n",
    "            programatic camera control<br/>\n",
    "            programatic saving<br/>\n",
    "            extensible</td>\n",
    "        <td> Requires local installation <br/>(no AWS possible)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> itkwidgets</td>\n",
    "        <td> fastest notebook widget <br/>\n",
    "             easy screen shot <br/>\n",
    "             bug free <br/>\n",
    "             aws compatible <br/>\n",
    "        </td>\n",
    "        <td>\n",
    "        no programatic camera control<br/>\n",
    "        no advanced mesh coloring\n",
    "        </td> \n",
    "    </tr>    \n",
    "    </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# viz_method = one of ['itkwidgets', 'vtk']\n",
    "viz_method = 'vtk'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Import the key modules</h2>\n",
    "Our analysis workflows use a couple of special purpose packages we have developed. Here, we are going to look at the CAVEclient (Connectome Annotation Versioning Engine), a package that quickly performs simple analysis queries and produces tidy Pandas dataframes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the EM specific package for querying the EM data\n",
    "from caveclient import CAVEclient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Import some of our favorite modules</h2>\n",
    "Our analysis workflows makes use of many of the common scientific computing packages like Numpy and Pandas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some of our favorite packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Setting up a CAVE account </h2>\n",
    "\n",
    "CAVE is the Connectome Annotation Versioning Engine.\n",
    "It is a collection of services which manage proofreading and data annotation for large scale connectomics dataset.\n",
    "These services were developed during the IARPA MICrONS program, but now support data from several projects including FANC and FlyWire.  You need to have an account on CAVE to use its services.\n",
    "\n",
    "\n",
    "To get an account please first go to this [website](https://cave.fanc-fly.com/materialize/views/datastack/fanc_production_mar2021)\n",
    "\n",
    "You should be prompted to login with a google account and then you must seperately accept the terms and conditions.\n",
    "\n",
    "Once you have registered, you need to get access to the dataset. If you have not already, please complete the [onboarding instructions](https://docs.google.com/document/d/10fnQHWTPluKeNXko6C63iKTKVwqXeSEQ10ftYgtEfhc/edit) of for using FANC. Once you have done so, please reach out to one of the following people with your email address to get access to the dataset:\n",
    "   \n",
    "* Jasper Phelps (jtmaniatesselvin@g.harvard.edu)\n",
    "* Leila Elabbady (elabbady@uw.edu)\n",
    "* John Tuthill (tuthill@uw.edu)\n",
    "* Wei-Chung Lee (wei-chung_lee@hms.harvard.edu)\n",
    " \n",
    "Once you have an account and have access to the FANC dataset, you can then follow along this notebook.\n",
    "This notebook will show you how to get a programatic token that you can use to authenticate to the services in order to make queries. The token can then be used on different computers.\n",
    "\n",
    "<h2>Setting key parameters </h2>\n",
    "Every combination of image and segmentation data we call a 'datastack', since a dataset might have multiple such 'datastacks' associated with it.  The female ventral nerve cord that we are going to use is called `fanc_v4` named because....\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CAVEclient()\n",
    "\n",
    "if not os.path.isfile(os.path.expanduser(\"~/.cloudvolume/secrets/cave-secret.json\")):\n",
    "    client.auth.get_new_token(open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not yet setup this computer, uncomment this below line\n",
    "# paste the token from the website in, and run the line\n",
    "\n",
    "# client.auth.save_token(token=\"paste_token_here\", overwrite=True)\n",
    "\n",
    "# then comment or delete the line as you don't need to run it on this computer  again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastack_name = 'fanc_production_mar2021'\n",
    "\n",
    "client = CAVEclient(datastack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Terms of Service Error</h2>\n",
    "If for some reason you are getting an error that says you haven't signed the terms of service, visit this link.\n",
    "\n",
    "<a href=\"https://global.daf-apis.com/sticky_auth/api/v1/tos/2/accept\"> Microns Public TOS </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Client Basics</h2>\n",
    "The client lets you connect to a number of different services, and you can read about all of them in  <a href=\"https://caveclient.readthedocs.io/\">the CAVEclient documentation</a>\n",
    "\n",
    "For this notebook however we will focus on the 'materialize' client which lets you access tables of annotations on the data, including cells, nuclei and synapses. \n",
    "\n",
    "To see what tables are available, use the 'get_tables' method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.materialize.get_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each table you can get metadata about that table\n",
    "client.materialize.get_table_metadata('soma_jan2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and how many annotations are in it\n",
    "# the maybe most important table is the synapse table (synapses_pni_2)\n",
    "# which has >330 million synapses, so querying its length takes a long time\n",
    "client.materialize.get_annotation_count('soma_jan2022')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the soma table\n",
    "# if you ask for the whole thing, the client will give you the first 200K rows\n",
    "# which is good enough to cover everything other than synapses\n",
    "soma_df= client.materialize.query_table('soma_jan2022')\n",
    "soma_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that you can check the metadata if you're unsure what is in the table\n",
    "client.materialize.get_table_metadata('soma_jan2022')['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful function for converting a pandas data frame voxel position\n",
    "# column to a np.array of Nx3 size in nm\n",
    "def convert_to_nm(col, voxel_size=[4.3,4.3,45]):\n",
    "    return np.vstack(col.values)*voxel_size\n",
    "\n",
    "# function to plot a dataframe\n",
    "def plot_soma_loc(df, ax, x=0, y=1, c='r', s=10, cmap=None):\n",
    "    \n",
    "    # convert the position to microns\n",
    "    pos = convert_to_nm(df['pt_position'])/1000\n",
    "    \n",
    "    if cmap:\n",
    "        c = pos[:,y]\n",
    "        ax.scatter(pos[:,x], pos[:,y], c=c, s=s)\n",
    "    # plot two dimensions as a scatterplot\n",
    "    else:\n",
    "        ax.scatter(pos[:,x], pos[:,y], c=c, s=s)\n",
    "\n",
    "# make a new axis with two subplots\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2, sharex=ax1)\n",
    "\n",
    "# plot each soma\n",
    "plot_soma_loc(soma_df, ax1, cmap='viridis', s=1)\n",
    "#plot_soma_loc(ext_axon_df, ax1, c='r')\n",
    "\n",
    "\n",
    "# label some axis, make x,y scaling same\n",
    "ax1.set_xlabel('x (um)')\n",
    "ax1.set_ylabel('y (um)')\n",
    "# to keep the pia 'up'\n",
    "#ax1.set_ylim(1100,300)\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# plot the same thing as x,z\n",
    "plot_soma_loc(soma_df, ax2, x=2, y=1, cmap='viridis', s=1)\n",
    "#plot_soma_loc(ext_axon_df, ax2, x=0, y=2, c='r')\n",
    "\n",
    "ax2.set_xlabel('z (um)')\n",
    "ax2.set_ylabel('y (um)')\n",
    "ax2.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Querying for synapses</h2>\n",
    "Let's pick out a cell ID of a tibia flexor motor neuron and then find all the synapses onto that neuron</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_neuron = 648518346503918688\n",
    "motor_neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Now we are ready to query synapses for this neuron. Because the analysis database can, in principle, handle multiple different sources of synapses, we have to specify which synapse table we want to use. The current table holding the most up to date automated synapse detection is called `synapses_may2021`.  The \"info\" service has this information however, so we don't need to remember it.\n",
    "\n",
    "Because synapses are such a common query, there is a wrapper method `synapse_query` which makes uses the default synapse table and simplifies the syntax. So, we can specify which synapses we get back from the table by setting the `post_ids` argument to a list of IDs, which makes a query that only returns synapses whose postsynaptic id is the one selected.  You can also filter by `pre_ids` which will do the same for the pre-synaptic side. Setting this will only return synapses that are from certain neurons.  Setting both `pre_ids` and `post_ids` will return only synapses that are from the `pre_ids` onto the `post_ids`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.materialize.synapse_table)\n",
    "post_synapse_df = client.materialize.synapse_query(post_ids = motor_neuron)\n",
    "# lets post the shape to see how many synapses we have\n",
    "print(post_synapse_df.shape)\n",
    "# and take a peak at the whole dataframe\n",
    "post_synapse_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h3> Synapse Table Descriptions</h3>\n",
    "Here's a breakdown of what each of those columns mean.\n",
    "</div>\n",
    "\n",
    "<table style={float:left}>\n",
    "    <tr>\n",
    "        <td><h4>column</h4></td>\n",
    "        <td><h4>description</h4></td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>id</td>\n",
    "        <td>The ID that is specific to this synapse annotation</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pre_pt_supervoxel_id</td>\n",
    "        <td>a bookkeeping column for the presynaptic side </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pre_pt_root_id</td>\n",
    "        <td>the ID of the cell on the presynaptic side</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>post_pt_supervoxel_id</td>\n",
    "        <td>Same bookkeeping column as pre_pt but for the post synaptic side.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>post_pt_root_id</td>\n",
    "        <td>Same as pre_pt but for the post synaptic side</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>score</td>\n",
    "        <td>The size of the synaptic cleft in units of 4.3,4.3,45 voxels.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pre_pt_position</td>\n",
    "        <td>a point that is in the pre-synaptic terminal of this synapse (in voxels) </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>post_pt_position</td>\n",
    "        <td>Same as pre_pt but for the post synaptic side.</td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Each row in this table is a single synapse. You should see that the value in the post_pt_root_id column is the same for all, and equals the id you selected above. \n",
    "<h5>Note again that position columns are in voxel coordinates, just like Neuroglancer displays in the upper left corner. A single voxel has dimensions 4.3x4.3x45 nm.</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a matrix of synapse positions for this neuron\n",
    "syn_pos_nm = convert_to_nm(post_synapse_df.post_pt_position)\n",
    "# and plot their positions in x,y along with the soma position as a red circle\n",
    "f ,ax =plt.subplots(1,2,figsize=(12,6))\n",
    "ax[0].scatter(syn_pos_nm[:,0]/1000, syn_pos_nm[:,1]/1000, alpha=0.6, s=5, c=post_synapse_df.score, cmap='terrain')\n",
    "\n",
    "# add the soma position as a red circle\n",
    "soma_pos = convert_to_nm(soma_df.pt_position[soma_df.pt_root_id == motor_neuron])\n",
    "ax[0].scatter([soma_pos[0,0]/1000],[soma_pos[0,1]/1000],c='r',s=150)\n",
    "ax[0].set_aspect('equal')\n",
    "ax[0].set_xlabel('x (um)')\n",
    "ax[0].set_ylabel('y (um)')\n",
    "\n",
    "\n",
    "m =ax[1].scatter(syn_pos_nm[:,0]/1000, syn_pos_nm[:,2]/1000, alpha=0.6, s=5, c=post_synapse_df.score, cmap='terrain')\n",
    "\n",
    "# add the soma position as a red circle\n",
    "soma_pos = convert_to_nm(soma_df.pt_position[soma_df.pt_root_id == motor_neuron])\n",
    "ax[1].scatter([soma_pos[0,0]/1000],[soma_pos[0,2]/1000],c='r',s=150)\n",
    "ax[1].set_aspect('equal')\n",
    "ax[1].set_xlabel('x (um)')\n",
    "ax[1].set_ylabel('z (um)')\n",
    "f.colorbar(m, label='Synapse Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Now lets figure out which premotor neuron makes the most synapses onto this tibia flexor\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas groupby to count number of synapses from different neurons\n",
    "# followed by transform to label the individual synapses with \n",
    "# how many other synapses are in that connection.\n",
    "\n",
    "# transform labels all the rows in the group with the result of this function on the group\n",
    "syn_in_conn=post_synapse_df.groupby('pre_pt_root_id').transform(len)['id']\n",
    "# save this result in a new colum\n",
    "post_synapse_df['syn_in_conn']=syn_in_conn\n",
    "post_synapse_df[['id', 'pre_pt_root_id', 'score','syn_in_conn']].sort_values('syn_in_conn', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Let's go ahead and get the ID of that premotor neuron which will have the largest number of synapses onto this tibia flexor\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the neuron with the most synapses\n",
    "max_input_idx = post_synapse_df.syn_in_conn.idxmax()\n",
    "max_input_neuron = post_synapse_df.loc[max_input_idx].pre_pt_root_id\n",
    "print(max_input_neuron)\n",
    "\n",
    "# the shape of this synapse table will tell us how many synapses are made by this prem\n",
    "post_synapse_df[post_synapse_df.pre_pt_root_id == max_input_neuron].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2>Linking synapses to somas</h2>\n",
    "You might want to link synapses to the cell bodies or cell classes of the pre or post-synaptic cells.  To do this, you want to 'merge' (See <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\"> pandas merge docs </a> ) the information between a synapse table and a table of somas. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging in the neuron table using 'left' ensures that every row\n",
    "# in the first dataframe passed (the synapse table) \n",
    "# has at least one entry in the result\n",
    "# more than 1 if there is more than one on the 'right'\n",
    "# (the second table passed)\n",
    "synapse_and_soma_df = pd.merge(post_synapse_df, soma_df,\n",
    "         left_on='post_pt_root_id',\n",
    "         right_on='pt_root_id',\n",
    "         how='left',\n",
    "         suffixes=['_syn', '_soma'])\n",
    "\n",
    "# now pt_position is the soma position\n",
    "# id_syn is the ID of the synapse\n",
    "# id_soma is the ID of the soma that had the matched root_id\n",
    "# pt_position is the position of that soma\n",
    "synapse_and_soma_df[['id_syn',\n",
    "                     'pre_pt_root_id',\n",
    "                     'post_pt_root_id',\n",
    "                     'id_soma',\n",
    "                     'pt_position']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3>Mesh visualization</h3>\n",
    "Now we can plot these synapses in 2d, but we have the detailed 3d morphology of these neurons, so why don't we look at them!\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an EM specific package for getting meshes\n",
    "# and doing analysis on those meshes\n",
    "from meshparty import trimesh_io, trimesh_vtk\n",
    "from meshparty import skeletonize, skeleton_io, skeleton\n",
    "import cloudvolume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "To access the 3d meshes of neurons, we need use a MeshMeta object, make sure to pass the segmentation source (which you can get from the client!) and the segmentation id that you are interested in. In our case, we want to see the motor neuron and the strongest premotor neuron\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "        \n",
    "seg_source = client.info.segmentation_source()\n",
    "\n",
    "mm = trimesh_io.MeshMeta(cv_path =seg_source,\n",
    "                         disk_cache_path='../meshes',map_gs_to_https=True)\n",
    "motor_neuron_mesh = mm.mesh(seg_id = motor_neuron, remove_duplicate_vertices=True, merge_large_components=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the meshes\n",
    "premotor_mesh = mm.mesh(seg_id = max_input_neuron,remove_duplicate_vertices=True, merge_large_components=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "Meshes are triangular meshes, that are composed of vertices (N,3) and faces (N,3) which are indices into the vertex list\n",
    "    \n",
    "\n",
    "The Mesh class is based upon trimesh (<a href=\"https://github.com/mikedh/trimesh\"> https://github.com/mikedh/trimesh</a>), with some added features for doing graphs\n",
    "\n",
    "You can find the source code for mesh party here .. <a href=\"https://github.com/sdorkenw/MeshParty\">https://github.com/sdorkenw/MeshParty </a>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_neuron_mesh.vertices.shape, motor_neuron_mesh.faces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Below we are going to visualize things in 3d.  Different code paths  illustrate different visualization methods\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set viewer equals None so vtk option doesn't error\n",
    "viewer = None\n",
    "if viz_method == 'itkwidgets':\n",
    "    #import ITK widgets view function\n",
    "    from itkwidgets import view\n",
    "    \n",
    "    # step 1\n",
    "    # convert your actors to vtkpolydata objects\n",
    "    post_poly_data = trimesh_vtk.trimesh_to_vtk(motor_neuron_mesh.vertices, motor_neuron_mesh.faces, None)\n",
    "    pre_poly_data = trimesh_vtk.trimesh_to_vtk(premotor_mesh.vertices, premotor_mesh.faces, None)\n",
    "\n",
    "    # step 2\n",
    "    # then create a viewer with this view function\n",
    "    # pass in polydata objects, what colors you want\n",
    "    # see docstring for more options\n",
    "    prepostviewer=view(geometries=[post_poly_data, pre_poly_data],\n",
    "                ui_collapsed=True)\n",
    "\n",
    "    # viewer controls..\n",
    "    # pinch movements:  to zoom in and out\n",
    "    # ctrl+wheel: on a mouse to do the same\n",
    "    # shift+drag: to pan\n",
    "\n",
    "    display(prepostviewer)\n",
    "elif viz_method == 'vtk':\n",
    "\n",
    "    # Step 1\n",
    "    # Convert meshes to actors, providing color and opacity\n",
    "    # options, you can provide vertex colors to color mesh vertices\n",
    "    # however these will only be relevant for vizmethod=vtk\n",
    "    post_actor = trimesh_vtk.mesh_actor(motor_neuron_mesh,\n",
    "                                    opacity=1.0,\n",
    "                                    color=(1,0,1))\n",
    "    pre_actor = trimesh_vtk.mesh_actor(premotor_mesh,\n",
    "                                   opacity=1.0,\n",
    "                                   color=(0,1,0))\n",
    "    print(\"A VTK window should have popped up behind you\")\n",
    "    print(\"WARNING YOU NEED TO CLOSE IT BY PRESSING Q TO MOVE ON\")\n",
    "    # step 2\n",
    "    # render them interactively with this function\n",
    "    # passing a list of actors\n",
    "    # can optionally specify a path to save a static image\n",
    "    trimesh_vtk.render_actors([pre_actor, post_actor])\n",
    "\n",
    "    # vtk controls\n",
    "    # pinch movements:  to zoom in and out\n",
    "    # ctrl+wheel: on a mouse to do the same\n",
    "    # shift+drag: to pan\n",
    "    # mouse over + f: to zoom to where you point and recenter camera there\n",
    "    # w: wireframe visualization\n",
    "    # s: surface visualization\n",
    "    # q: to exit visualization\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in vtk and vtkplotter there is more camera control\n",
    "# so this is how you would automatically center the camera\n",
    "# on the first synapse between these neurons\n",
    "max_synapses = post_synapse_df[post_synapse_df.pre_pt_root_id == max_input_neuron]\n",
    "\n",
    "# get the location of the first synapse (change 0 to 1 or 2 to look at others)\n",
    "syn_pos =convert_to_nm(max_synapses.iloc[[0]].post_pt_position)\n",
    "\n",
    "# create a camera object pointed at the synapse\n",
    "camera = trimesh_vtk.oriented_camera(syn_pos, backoff=20)\n",
    "\n",
    "\n",
    "if viz_method == 'vtk':\n",
    "    print(\"A VTK window should have popped up behind you\")\n",
    "    print(\"WARNING YOU NEED TO CLOSE IT BY PRESSING Q TO MOVE ON\")\n",
    "    # pass the camera to the render_actors function to control camera\n",
    "    trimesh_vtk.render_actors([post_actor, pre_actor], camera=camera)\n",
    "if viz_method == 'itkwidgets':\n",
    "    print(\"this should have changed the colors and the camera position in the widget above\")\n",
    "    prepostviewer.geometry_colors = np.array([[.8,0,.8], [0,1,0]], dtype=np.float32)\n",
    "    prepostviewer.camera = camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Visualizing synapses </h3>\n",
    "Now what if we just want to see the synapse locations without the mesh of the other side\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# synapse sizes tend to distributed widely\n",
    "# so we are going to color and size them proportional to the log\n",
    "syn_color = np.log(post_synapse_df['score'].values)\n",
    "syn_size = 100*np.log(post_synapse_df['score'].values)\n",
    "\n",
    "# this will produce spheres at each point with sizes in nm\n",
    "# and color values that are mapped through a colormap\n",
    "# in VTK only you can pass explicit colors as well\n",
    "syn_actor = trimesh_vtk.point_cloud_actor(syn_pos_nm, size=syn_size, color=syn_color)\n",
    "\n",
    "if viz_method == 'vtk':\n",
    "    print('remember the window pops up behind')\n",
    "    trimesh_vtk.render_actors([post_actor, syn_actor])\n",
    "if viz_method == 'itkwidgets':\n",
    "    syn_pd = syn_actor.GetMapper().GetInput()\n",
    "    synviewer = view(geometries=[post_poly_data, syn_pd],\n",
    "                  geometry_colors=['m','g'],\n",
    "                  ui_collapsed=True)\n",
    "    display(synviewer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Working with Skeletons </h3>\n",
    "    \n",
    "Just as the 3D mesh of the neuron is a reduced data representation of the imagery, one can further reduce a 3D mesh into a skeleton format which can benefit certain types of analyses by making it faster/ more optimal. The <a href = \"https://meshparty.readthedocs.io/en/latest/guide/skeletons.html\" >\"skeletonize\"</a> library within meshparty allows us to generate these skeletons from a mesh. Note that this algorithm generates a skeleton where skeleton points generated are a subset of the mesh points. For a few cells however, we have already generated these and provide this in the data directory.\n",
    "    \n",
    "The Meshwork library allows us to easily store these representations and helps us relate them to each other. A Meshwork object is a data structure that is designed to have three main components that are kept in sync with mesh and skeleton indices: \n",
    "    \n",
    "    mesh: a standard meshparty mesh\n",
    "    skeleton: a standard meshparty skeleton,\n",
    "    anno : is a class that holds dataframes and adds some extra info to keep track of indexing. \n",
    "    \n",
    "In addition, the meshwork object itself has a number of skeleton-like functions that know when to use mesh data and when to use skeleton data. For the most part of this tutorial, we will focus on using the skeleton object.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshparty import meshwork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Attributes of meshworks objects:\n",
    "    \n",
    "    nrn.mesh, nrn.skeleton, nrn.anno\n",
    "\n",
    "    \n",
    "Let us now try to visualize the skeleton: nrn.skeleton:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = convert_to_nm(soma_df.pt_position[soma_df.pt_root_id == motor_neuron])[0]\n",
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert our mesh object into a meshwork object\n",
    "motor_neuron_mw = meshwork.Meshwork(motor_neuron_mesh, seg_id=motor_neuron,voxel_resolution=[4.3,4.3,45])\n",
    "\n",
    "#create a skeleton for this neuron\n",
    "motor_neuron_mw.skeletonize_mesh(soma_pt=soma_point)\n",
    "\n",
    "# #since we have them, let's add the synapses onto this neuron as an annotation \n",
    "motor_neuron_mw.add_annotations('post_synapses', post_synapse_df, point_column='post_pt_position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us try to visualize the skeleton:\n",
    "# Visualize the whole skeleton \n",
    "skeleton_actor= trimesh_vtk.skeleton_actor(motor_neuron_mw.skeleton,color=(0.5,0,0.2))\n",
    "if viz_method=='vtk':\n",
    "    \n",
    "    trimesh_vtk.render_actors([skeleton_actor])\n",
    "elif viz_method=='itkwidgets':\n",
    "    sk_pd = skeleton_actor.GetMapper().GetInput()\n",
    "    skviewer = view(geometries=[sk_pd],\n",
    "                  geometry_colors=['k'],\n",
    "                  ui_collapsed=True)\n",
    "    display(skviewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3>Indexing And Selecting Sets of Interesting Points</h3>\n",
    "\n",
    "Ok, you are probably really fried by now, but here is one important thing to remember about this meshworks object. The meshworks object contains a mesh with lots of vertices and a skeleton which holds a subset of these vertices. Therefore, in python these points have different \"indices\" in the mesh and skeleton. For example, if the mesh contains 10000 vertices, the indexing of those would run from 0 - 9999. The skeleton, which contains a subset of 100 of these would have indexing from 0-99. How would you figure out which of the mesh vertices these correspond to?\n",
    "    \n",
    "Luckily, we have some really nifty functions that help us distinguish those:\n",
    "    \n",
    "Let us first look at some attributes in the meshworks objects:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch points\n",
    "print(\"Branch points with mesh indices: \\n\", nrn.branch_points)\n",
    "print(\"Branch points with skeleton indices: \\n\",nrn.branch_points_skel)\n",
    "\n",
    "# nrn.branch_points returns a list of branch points of the cell as a mesh index.\n",
    "# What it's actually doing is finding the branch points of the skeleton, \n",
    "# then looking up a single mesh vertex that corresponds to that skeleton vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End points\n",
    "print(\"End points with mesh indices: \\n\", nrn.end_points)\n",
    "print(\"End points with skeleton indices: \\n\",nrn.end_points_skel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Root - point associated with the root node\n",
    "print(\"Root point with mesh indices: \", nrn.root)\n",
    "print(\"Root point with skeleton indices: \",nrn.root_skel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    Finally One last nifty function for subselecting points: downstream points.\n",
    "    For a given point, downstream points are defined as points on paths from endpoints to the root which are further than the given point. For example, if the skeleton path is : A-B-C-D-E where A is the root, D and E are downstream points of C. With branching, this can be more complex. To find the downstream points from say the 9th branch point, we can do:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downstream points\n",
    "\n",
    "nrn.downstream_of(nrn.branch_points[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Masking</h3>\n",
    "Just like meshparty meshes, we can mask the meshwork object. Like all basic meshwork functions, the expected input is in mesh vertices. Importantly, doing so will be synchronized across the mesh, the skeleton, and annotations. Do not use the nrn.mesh.apply_mask or nrn.skeleton.apply_mask functions\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrn.reset_mask() #VERY IMPORTANT - needed to ensure the mask query is run on the whole object\n",
    "\n",
    "full_object_skeleton = trimesh_vtk.skeleton_actor(nrn.skeleton, color=(0.5,0.5,0.5), line_width=1, opacity=0.5, )\n",
    "\n",
    "nrn.apply_mask(nrn.downstream_of(nrn.branch_points[7]).to_mesh_mask)\n",
    "\n",
    "masked_object_skeleton = trimesh_vtk.skeleton_actor(nrn.skeleton, color=(1,0,0), line_width=4, opacity=0.5, )\n",
    "nrn.reset_mask() # if you are going to do any more operations after this, make sure this is reset\n",
    "if viz_method=='vtk': \n",
    "    trimesh_vtk.render_actors([full_object_skeleton, masked_object_skeleton])\n",
    "elif viz_method=='itkwidgets':\n",
    "    fos_pd = full_object_skeleton.GetMapper().GetInput()\n",
    "    mos_pd = masked_object_skeleton.GetMapper().GetInput()\n",
    "    viewer = view(geometries=[fos_pd, mos_pd], ui_collapsed=True)\n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with itkwidgets you have to set colors after setting up rendering\n",
    "# due to a bug, the widget should change colors when you do this\n",
    "if viz_method=='itkwidgets':\n",
    "    viewer.geometry_colors = np.array([[0,0,0], [1,0,0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Annotations</h3>\n",
    "`nrn.anno` has set of dataframes containing some additional information for analysis. To find out what information it contains, look at the table_names attribute: \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(nrn.anno.table_names)\n",
    "#This shows that there are four dataframes:\n",
    "#'is_axon' is the annotation of mesh vertices on the axon\n",
    "#'lvl2_ids' gives the annotation of corresponding level 2 ids\n",
    "#'post_syn' is the annotation of synapses for which this cell is the postsynaptic cell\n",
    "#'pre_syn' is the annotation of synapses for which this cell is the presynaptic cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "How do annotations work?\n",
    "\n",
    "Let's look at the pre_syn annotation. You can access the data as a dataframe with `.df`\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrn.anno.pre_syn.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "The in-built properties and functions of a meshworks object allow you to connect the dataframe with mesh/skeleton information and operations. \n",
    "    \n",
    "For example, you can get the mesh index for each row with `.mesh_index`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrn.anno.pre_syn.mesh_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    " \n",
    "And the skeleton index for each row with `.skel_index`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrn.anno.pre_syn.skel_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "These mesh indices can be linked up to the meshwork class functions trivially using this information:\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrn.distance_to_root(nrn.anno.pre_syn.mesh_index)/1000 # in microns\n",
    "\n",
    "#This gives the distance from each point to the root in microns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Finally, we will learn how to filter and mask these results for targeted analysis\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrn.anno.pre_syn.filter_query( nrn.anno.is_axon.mesh_mask).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e17c704ee36528c3cfcd7dcc3f7e33facab0a30a3c4c97ac2d81f69379006198"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
