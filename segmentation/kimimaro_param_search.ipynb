{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing parameters for Kimimaro skeletonization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import Bbox, Vec, CloudVolume, volumecutout, view\n",
    "import kimimaro\n",
    "from meshparty import trimesh_io, trimesh_vtk, skeleton_io\n",
    "import navis\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from fanc_seg_utils import neuroglancer_utilities\n",
    "from fanc_seg_utils import skeletonization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a point inside a neuron to analyze, download a region to skeletonize, and download the mesh for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mip = np.array([4.3,4.3,45])\n",
    "seg_mip = np.array([17.2,17.2,45])\n",
    "\n",
    "vol = CloudVolume('https://storage.googleapis.com/zetta_lee_fly_vnc_001_segmentation/vnc1_full_v3align_2/realigned_v1/seg/full_run_v1',parallel=True,progress=True,mip=seg_mip)\n",
    "\n",
    "## Select point and get associated seg_id and mesh\n",
    "pt = np.array([19931, 113919, 2581]) \n",
    "seg = neuroglancer_utilities.seg_from_pt(pt)\n",
    "mesh = vol.mesh.get(seg) \n",
    "\n",
    "## Download point, make sure it is good\n",
    "label = vol.download_point(pt // np.array([4, 4, 1]) ,size=512, mip = seg_mip)\n",
    "plt.imshow(label[:,:,256])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for searching.  Output is in navis neurons for plotting/manipultion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_search(label,seg,img_res,**kwargs):\n",
    "    teasar_params={\n",
    "    'scale': 1,\n",
    "    'const': 200, # physical units\n",
    "    'pdrf_exponent': 4,\n",
    "    'pdrf_scale': 100000,\n",
    "    'soma_detection_threshold': 1100, # physical units\n",
    "    'soma_acceptance_threshold': 3500, # physical units\n",
    "    'soma_invalidation_scale': 1.0,\n",
    "    'soma_invalidation_const': 300, # physical units\n",
    "    'max_paths': 75, # default None\n",
    "  }\n",
    "    for i in teasar_params.keys():\n",
    "        if i in kwargs.keys():\n",
    "            teasar_params[i] = kwargs[i]\n",
    "    \n",
    "    if 'dust_threshold' in kwargs.keys():\n",
    "        dust_threshold = kwargs['dust_threshold']\n",
    "    else:\n",
    "        dust_threshold = 500\n",
    "        \n",
    "        \n",
    "    skels = kimimaro.skeletonize(\n",
    "      label,teasar_params=teasar_params,\n",
    "      object_ids=[seg], # process only the specified labels\n",
    "      # extra_targets_before=[ (27,33,100), (44,45,46) ], # target points in voxels\n",
    "      # extra_targets_after=[ (27,33,100), (44,45,46) ], # target points in voxels\n",
    "      dust_threshold = dust_threshold, # skip connected components with fewer than this many voxels\n",
    "      anisotropy=(17,17,45), # default True\n",
    "      fix_branching=True, # default True\n",
    "      fix_borders=True, # default True\n",
    "      fill_holes=False, # default False\n",
    "      fix_avocados=False, # default False\n",
    "      progress=True, # default False, show progress bar\n",
    "      parallel=0, # <= 0 all cpu, 1 single process, 2+ multiprocess\n",
    "      parallel_chunk_size=100, # how many skeletons to process before updating progress bar\n",
    "    )\n",
    "    \n",
    "    for i in kwargs.keys():\n",
    "        param = i\n",
    "        val = kwargs[i]\n",
    "        print(val)\n",
    "    \n",
    "    neurons = []    \n",
    "    for i in skels.keys():\n",
    "        sk = skels[i]\n",
    "       # sk.vertices = ( (sk.vertices ) + (np.array(label.bounds.to_list()[0:3]) * multiplier ))\n",
    "        sk.vertices =  (sk.vertices) + (np.array(label.bounds.to_list()[0:3]) * np.array([4,4,1])) * img_res\n",
    "        neuron = navis.TreeNeuron(skeletonization.km_to_swc(sk,radius=True,xyz_scaling=1))\n",
    "        neuron.name = param + ':' + str(val) \n",
    "        neuron.nodes.radius = 0\n",
    "        neuron.nodes[['x','y','z']] = np.array(neuron.nodes[['x','y','z']])\n",
    "        neurons.append(neuron)\n",
    "\n",
    "\n",
    "    \n",
    "    return(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_search = []\n",
    "for i in range(10):\n",
    "    skeleton_search.append(param_search(label,seg,img_mip,dust_threshold=(i*100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_n = navis.MeshNeuron(mesh)\n",
    "colors = plt.cm.plasma(np.linspace(0,1,11))\n",
    "\n",
    "navis.plot3d([mesh_n,skeleton_search],alpha = .1,linewidth=5,c=colors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectomics_analysis",
   "language": "python",
   "name": "connectomics_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
