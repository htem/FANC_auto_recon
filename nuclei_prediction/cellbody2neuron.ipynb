{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predict parent neuron\n",
    "Using Jasper's idea to predict parent neurons for nuclei, Thank you so much, Jasper."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.j-focus.jp/user_guide/ug0004040000/\n",
    "sbatch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# libraries 1\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from cloudvolume import CloudVolume, view, Bbox\n",
    "import cc3d\n",
    "from tifffile.tifffile import imwrite\n",
    "import fill_voids\n",
    "from taskqueue import TaskQueue, queueable\n",
    "from functools import partial\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# libraries 2\n",
    "# %pwd\n",
    "sys.path.append(os.path.abspath(\"../segmentation\"))\n",
    "# to import rootID_lookup and authentication_utils like below\n",
    "import rootID_lookup as IDlook\n",
    "import authentication_utils as auth"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser(description='get segIDs of parent neurons from csv files') \n",
    "parser.add_argument('-c', '--choose', help='specify the numer of pixels randomly chosen to get segID of parent neuron. default is all surroundinx pixels', default=0, type=int)\n",
    "parser.add_argument('-l', '--lease', help='lease_seconds for TaskQueue.poll. specify in seconds. default is 600sec', default=600, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "choose=args.choose\n",
    "lease=args.lease"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "np.random.seed(123)\n",
    "# queuepath = '/n/groups/htem/users/skuroda/nuclei_tasks'\n",
    "queuepath = '../Output/nuclei_tasks'\n",
    "size_xy = 160 # 160/(2**2)??\n",
    "# 128x128x160 is small"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# cv setting\n",
    "seg = CloudVolume(auth.get_cv_path('FANC_production_segmentation')['url'], use_https=True, agglomerate=False, cache=True, progress=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# read csv\n",
    "df = pd.read_csv('../Output/info_cellbody.csv', header=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_task_queue()\n",
    "run_tasks_from_queue()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# give me top3, ouput csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def vol_shift(input): # Although np.roll is fast, this is very slow since this overuse RAM\n",
    "    # x plane\n",
    "    x_p = np.roll(input, 1, axis=0)\n",
    "    x_p[0,:,:] = 0\n",
    "    x_n = np.roll(input, -1, axis=0)\n",
    "    x_n[-1,:,:] = 0\n",
    "    # y plane\n",
    "    y_p = np.roll(input, 1, axis=1)\n",
    "    y_p[:,0,:] = 0\n",
    "    y_n = np.roll(input, -1, axis=1)\n",
    "    y_n[:,-1,:] = 0\n",
    "    # z plane\n",
    "    z_p = np.roll(input, 1, axis=2)\n",
    "    z_p[:,:,0] = 0\n",
    "    z_n = np.roll(input, -1, axis=2)\n",
    "    z_n[:,:,-1] = 0\n",
    "\n",
    "    sum = x_p + x_n + y_p + y_n + z_p + z_n\n",
    "    result = sum - input*6\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# global variable is pt, segid, sizexy, choose\n",
    "\n",
    "@queueable\n",
    "def task_cellbody2neuron(i):\n",
    "  cord_mip0 = df.iloc[i,0:3] #xyz coordinates\n",
    "  cord_mip2 = cord_mip0.values # change coordination from mip0 to mip2\n",
    "  cord_mip2[0]  = (cord_mip0.values[0] /(2**2))\n",
    "  cord_mip2[1]  = (cord_mip0.values[1] /(2**2))\n",
    "  cord_mip2 = cord_mip2.astype('int64')\n",
    "  id = df.iloc[i,3] #segid\n",
    "\n",
    "  seg_nuc = seg.download_point(pt=cord_mip2, segids=id, size=[size_xy, size_xy, 160], coord_resolution=[17.2, 17.2, 45.0])\n",
    "  # lowest resolution of seg is [17.2, 17.2, 45.0]\n",
    "  vol_temp = seg_nuc[:,:,:]\n",
    "  vol_temp[vol_temp>0] = 1 # change segID assigned to each cell body into 1\n",
    "  vol = np.squeeze(vol_temp)\n",
    "\n",
    "  filled = fill_voids.fill(vol, in_place=False) # fill the empty space with one \n",
    "  # ignore warning\n",
    "\n",
    "  shifted = vol_shift(filled) # shift the volume\n",
    "  shifted = shifted.astype('float32')\n",
    "  shifted[shifted>0] = 1\n",
    "  shifted[shifted<0] = 0\n",
    "\n",
    "  location_one = np.argwhere(shifted == 1)\n",
    "  len(location_one)\n",
    "\n",
    "  if len(location_one):\n",
    "    origin = seg_nuc.bounds.minpt # 3072,5248,1792\n",
    "    parent_coordinates_mip2 = np.add(np.array(location_one), origin)\n",
    "    parent_coordinates = parent_coordinates_mip2\n",
    "    parent_coordinates[:,0]  = (parent_coordinates_mip2[:,0] * 2**2)\n",
    "    parent_coordinates[:,1]  = (parent_coordinates_mip2[:,1] * 2**2)\n",
    "    parent_coordinates = parent_coordinates.astype('int64')\n",
    "\n",
    "    #random selection?\n",
    "    if choose == 0:\n",
    "      location_random = parent_coordinates\n",
    "    else:\n",
    "      index = np.random.choice(parent_coordinates.shape[0], size=choose, replace=False)\n",
    "      location_random = parent_coordinates[index]\n",
    "\n",
    "    # Lets get IDs using cell_body_coordinates\n",
    "    parent_IDs = IDlook.segIDs_from_pts_cv(pts=location_random, cv=seg) #mip0\n",
    "\n",
    "    # save\n",
    "    # type(cell_body_coordinates.shape)\n",
    "    # cord_pd = pd.DataFrame(cell_body_coordinates, columns=[\"x\", \"y\", \"z\"])\n",
    "    # temp = cord_pd\n",
    "    # temp['segIDs'] = cell_body_IDs\n",
    "    # output.append(temp)\n",
    "  else:\n",
    "    pass\n",
    "  \n",
    "  seg_nuc.cache.flush()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#task queue\n",
    " \n",
    "def create_task_queue():\n",
    "    tq = TaskQueue('fq://' + queuepath)\n",
    "    tq.insert(( partial(task_cellbody2neuron, i) for i in range(len(df)) )) # NEW SCHOOL?\n",
    "    tq.execute()\n",
    "    print('Done adding {} tasks to queue at {}'.format(len(df), queuepath))\n",
    "\n",
    "\n",
    "def run_tasks_from_queue():\n",
    "    tq = TaskQueue('fq://' + queuepath)\n",
    "    print('Working on tasks from filequeue \"{}\"'.format(queuepath))\n",
    "    tq.poll(\n",
    "        verbose=True, # prints progress\n",
    "        lease_seconds=int(lease),\n",
    "        tally=True # makes tq.completed work, logs 1 byte per completed task\n",
    "    )\n",
    "    print('Done')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3593f56db3fdc375a231e1c1fb5fc1dbab03dc3894478fa0aaa4bb9b486beca"
  },
  "kernelspec": {
   "display_name": "nuclei",
   "language": "python",
   "name": "nuclei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}