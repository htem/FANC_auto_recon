{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cloudvolume import CloudVolume\n",
    "import json\n",
    "from annotationframeworkclient import FrameworkClient\n",
    "import nglui\n",
    "from concurrent import futures\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to the annotation engine:\n",
    "#### 1. Get a JSON state\n",
    "#### 2. Upload neurons or synapses\n",
    "---\n",
    "##### Token for V1 chunkedgraph on dev: https://dev.zetta.ai/wclee/auth/api/v1/refresh_token\n",
    "##### Token for annotation framework from here: https://api.zetta.ai/wclee/auth/api/v1/refresh_token\n",
    "- These tokens are currently stored in ~/cloudvolume/secrets/authtokens.json, you should put them here too since this folder will be used by other things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path.home() / 'cloudvolume' / 'secrets'/'chunkedgraph-secret.json') as f:\n",
    "        tokens = json.load(f)\n",
    "\n",
    "# Tokens from above links passed as strings. \n",
    "dev_token = tokens['dev']\n",
    "auth_token = tokens['api']\n",
    "\n",
    "datastack_name = 'vnc_v0' # from https://api.zetta.ai/wclee/info/\n",
    "\n",
    "client = FrameworkClient(\n",
    "    datastack_name,\n",
    "    server_address = \"https://api.zetta.ai/wclee\",\n",
    "    auth_token = auth_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the JSON state from a NG instance containing an annotation layer.\n",
    "- JSON states are the primary means of managing things.\n",
    "- When working in neuroglancer, to save the state of your workspace, press ctrl-shift-j. This will change the url with the /xxxxx being the JSON state ID. These states are stored in the JSON state service of the annotation framework. They are NOT EASY to look up, so here is a method for managing them.\n",
    "- Define a state manager to keep track of our states. Save the state dataframe to the ~/.cloudvolume. We will put this in a module later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager: \n",
    "    \n",
    "    ''' Class for keeping track of JSON states.'''\n",
    "    def __init__(self,\n",
    "                 filename=None,\n",
    "                 token=None):\n",
    "        \n",
    "        self.directory = Path.home() / 'cloudvolume'\n",
    "        if filename is None:\n",
    "            self.filename = self.directory / 'json_states.csv'\n",
    "        \n",
    "        self.__initialize()\n",
    "        \n",
    "        \n",
    "    def __initialize(self):\n",
    "        # Check if the database exists, if not create a new one.\n",
    "        fileEmpty =  os.path.exists(self.filename)\n",
    "        if not fileEmpty:\n",
    "            df = pd.DataFrame(columns=['state_id','description'])\n",
    "            df.to_csv(self.filename,index=False)\n",
    "        self.get_database()\n",
    "        print(self.df) \n",
    "    \n",
    "    def get_database(self):\n",
    "        # Read database. \n",
    "        self.df  = pd.read_csv(self.filename)\n",
    "        \n",
    " \n",
    "    def add_state(self, state_id, description=None):\n",
    "        \n",
    "        filename = self.filename\n",
    "        df = pd.DataFrame([{'state_id':state_id,'description':description}])\n",
    "        df.to_csv(filename, mode='a', header=False,index=False, encoding = 'utf-8')\n",
    "        self.get_database()\n",
    "        return('state added')\n",
    " \n",
    "    def get_state(self,state_id):\n",
    "        return(client.state.get_state_json(state_id))\n",
    "    \n",
    "    def get_url(self,state):\n",
    "        return('https://neuromancer-seung-import.appspot.com/?json_url=https://api.zetta.ai/json/' + str(state))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the state ID you want, or add a state with `sm.add_state()`\n",
    "#### Next, get the JSON state with `sm.get_state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             state_id                                       description\n",
      "0  261002187096550146                                    MNs,10Bs,Claws\n",
      "1  838783366227209333  Slow Tibia Flexor in V1 chunkedgraph and V2 flat\n",
      "2  838783366227209333  Slow Tibia Flexor in V1 chunkedgraph and V2 flat\n",
      "3  585521373111957130              Comparison of V1 and V2 chunkedgraph\n",
      "4  758492429940665723                                     9A_T1 neurons\n",
      "5  299984539931822600                                              PMNs\n",
      "6  336757619423424673                          81A07 synapse annotation\n",
      "https://neuromancer-seung-import.appspot.com/?json_url=https://api.zetta.ai/json/336757619423424673\n"
     ]
    }
   ],
   "source": [
    "sm = StateManager()\n",
    "print(sm.get_url(336757619423424673))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sm.get_state(sm.df.state_id[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at available layers\n",
    "- If you had annotation layers in your state, marking either synapses or cells, they will be shown here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['realigned_v1', 'vnc1_full_v3align_2', 'region meshes', 'Synapses']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nglui.parser.layer_names(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some methods for formatting schemas. \n",
    "##### We are going to use two schemas for now. \n",
    "1. The synapse schema for synapses.\n",
    "    - To generate synapse entries, use an annotation layer that used line annotations with pt1 being the presynapse and pt2 being the postsynspse. \n",
    "2. The bound_tag schema for marking cells. \n",
    "    - To generate cell entreis, use an annotation layer that used point annotations, preferably on the soma (but not in the nucleus unless you merged the nucleus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soma_table_entries(state,layer_name = 'cells'):\n",
    "    ''' Generate entries for a soma table using the bound_tag schema \n",
    "    Args:\n",
    "    state: json, json state from get_json_state\n",
    "    layer_name: name of layer containing soma coords\n",
    "    ### Change this to points later. This is just for database copying because I used a stupid annotation\n",
    "    \n",
    "    Returns:\n",
    "    entries: list, list of dict entries for the bound_tag schema'''\n",
    "    \n",
    "    cell_layer = nglui.parser.get_layer(state,layer_name)\n",
    "    entries = []\n",
    "    for i in cell_layer['annotations']:\n",
    "        entry = {'tag': i['description'],\n",
    "                 'pt': {'position': i['pointA']}}\n",
    "        entries.append(entry)\n",
    "    return(entries)\n",
    "        \n",
    "\n",
    "def upload_cells(cell_entries,table_name,description=None):\n",
    "    ''' Upload cell entries to a soma dable using the bound_tag schema'\n",
    "    Args:\n",
    "    cell_entries: list, list of dicts from soma_table_entries\n",
    "    table_name: str, table name to create. If it exists, it will populate the existing one. \n",
    "    description: str, description to supply if creating a new table. It will fail if you don't provide one. \n",
    "    \n",
    "    ##TODO: Some sort of check for redundant synapses is necessary. We could make each upload need to go to a new table, but that seems absurd. \n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        client.annotation.create_table(table_name=table_name,\n",
    "                                   schema_name='bound_tag',\n",
    "                                   description=description)\n",
    "    except:\n",
    "        print('table exists')\n",
    "        \n",
    "    for i in cell_entries:\n",
    "        try:\n",
    "            client.annotation.post_annotation(table_name=table_name, data=[i])\n",
    "        except:\n",
    "            print('Fail',entry)\n",
    "\n",
    "\n",
    "\n",
    "def get_synapses(state,synapse_layer='synapses'):\n",
    "    ''' Get the synapse coordinates from a json state and return pre,post,center.\n",
    "    Args:\n",
    "        state: json,  json state from get_state_json \n",
    "        synapse_layer: str, name of layer containing synapses in the json state. Default is 'synapses'\n",
    "    Returns:\n",
    "        pre_pt,post_pt,ctr_pt: list, lists of coordinates for synapses.'''\n",
    "    \n",
    "    \n",
    "    syns = nglui.parser.line_annotations(state,synapse_layer)\n",
    "    if np.shape(syns)[0] != 2:\n",
    "        raise Exception( print('Incorrectly formatted synapse annotation. Requires two lists: Presynapse coordinates, Postsynapse coordinates'))\n",
    "           \n",
    "    else:\n",
    "        pre_pt = syns[0]\n",
    "        post_pt = syns[1]\n",
    "        ctr_pt = (np.array(pre_pt) + np.array(post_pt)) / 2\n",
    "    return(pre_pt,post_pt,np.ndarray.tolist(ctr_pt))\n",
    "\n",
    "\n",
    "def format_synapse(pre,post,ctr):\n",
    "    ''' Format a synapse for upload to a synapse table\n",
    "    Args:\n",
    "           pre: list, mip0 xyz coordinates to presynapse\n",
    "           post: list, mip0 xyz coordinates to postsynapse\n",
    "           ctr: list, mip0 xyz coordinates to center point\n",
    "    Returns:\n",
    "           data: formatted dict for a synapse table annotation upload\n",
    "    '''\n",
    "    data = {\n",
    "    \"type\": \"synapse\",\n",
    "    'pre_pt': {'position': pre},\n",
    "    'post_pt': {'position': post},\n",
    "    'ctr_pt': {'position': ctr}\n",
    "\n",
    "}\n",
    "    return(data)\n",
    "\n",
    "def upload_synapses(synapse_coordinates,table_name,description=None):\n",
    "    ''' Add synapses to a synapse table.\n",
    "    Args: \n",
    "        synapse_coordinates: list, list of shape [3,n,3]. Dim 0 is [pre,post,ctr], Dim 1 is the entry, Dim 2 is [x,y,z]. Output of get_synapses\n",
    "    ##TODO: Some sort of check for redundant synapses is necessary. We could make each upload need to go to a new table, but that seems absurd. \n",
    "    '''\n",
    "    try:\n",
    "        client.annotation.create_table(table_name=table_name,\n",
    "                                   schema_name='synapse',\n",
    "                                   description=description)\n",
    "    except:\n",
    "        print('table exists')\n",
    "        \n",
    "    for i in range(np.shape(synapse_coordinates)[1]):\n",
    "        try:\n",
    "            entry = format_synapse(synapse_coordinates[0][i],synapse_coordinates[1][i],synapse_coordinates[2][i])\n",
    "            client.annotation.post_annotation(table_name=table_name, data=[entry])\n",
    "        except:\n",
    "            print('Fail',entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a table from an annotation layer\n",
    "##### There currently are not checks on uploading duplicate data, so do not go crazy with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'Synapses'\n",
    "table_name = 'Synapse_Table'\n",
    "description = 'Table of synapses for 81a07 gal4'\n",
    "\n",
    "# FOR CELLS\n",
    "#entries = soma_table_entries(state,layer_name=layer_name)\n",
    "#upload_cells(entries,table_name,description=description)\n",
    "\n",
    "# FOR SYNAPSES\n",
    "entries = get_synapses(state,synapse_layer=layer_name)\n",
    "upload_synapses(entries,tabe_name,description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some methods for working around the materialization engine.\n",
    "    1. Download an annotation table\n",
    "    2. Look up the root_ids associated with the points\n",
    "    3. Generate a dataframe that looks like the microns format (post materialization?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_annotation_table(table_name,ids=range(1000)):\n",
    "    entries = client.annotation.get_annotation(table_name,ids)\n",
    "    annotation_table = pd.DataFrame(entries)\n",
    "    return(annotation_table)\n",
    "\n",
    "\n",
    "def seg_from_pt(pts,vol,image_res=np.array([4.3,4.3,45]),max_workers=4):\n",
    "    ''' Get segment ID at a point. Default volume is the static segmentation layer for now. \n",
    "    Args:\n",
    "        pts (list): list of 3-element np.arrays of MIP0 coordinates\n",
    "        vol_url (str): cloud volume url\n",
    "    Returns:\n",
    "        list, segment_ID at specified point '''\n",
    "    \n",
    "    \n",
    "    seg_mip = vol.scale['resolution']\n",
    "    res = seg_mip / image_res\n",
    "\n",
    "    pts_scaled = [pt // res for pt in pts]\n",
    "    results = []\n",
    "    with futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        point_futures = [ex.submit(lambda pt,vol: vol[list(pt)][0][0][0][0], k,vol) for k in pts_scaled]\n",
    "        \n",
    "        for f in futures.as_completed(point_futures):\n",
    "            results=[f.result() for f in point_futures]\n",
    "       \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def generate_soma_table(annotation_table,\n",
    "                        segmentation_version='Dynamic_V1',\n",
    "                        resolution=np.array([4.3,4.3,45]),\n",
    "                        token=None):\n",
    "    ''' Generate a soma table used for microns analysis. This is the workaround for a materialization engine\n",
    "    Args:\n",
    "        annotation_table: pd.DataFrame, output from download_cell_table. Retreived from the annotation engine.\n",
    "        segmentation_version: str, Currently we have 4 for FANC. Two flat segmentations (\"Flat_1\" and \"Flat_2\") and two dynamic (\"Dynamic_V1/V2\"). \n",
    "                              This will only work if you have a segmentations.json in your cloudvolume folder. See examples for format.\n",
    "        resolution: np.array, Resolution of the mip0 coordinates of the version (not necessarily the same as the segmentation layer resolution).\n",
    "                              For all but the original FANC segmentation, this will be [4.3,4.3,45]\n",
    "        token: str, currently, CloudVolume requires a workaround for passing google secret tokens. This won't work unless you edit your cloudvolume \n",
    "                              file to remove the check for hexidecimal formatting of tokens. Updates should be coming to fix this. \n",
    "        '''\n",
    "\n",
    "    soma_table = pd.DataFrame(columns=['name','cell_type',\n",
    "                                       'pt_position','pt_root_id',\n",
    "                                       'soma_x_nm','soma_y_nm','soma_z_nm',\n",
    "                                       'found'])\n",
    "    with open(Path.home() / 'cloudvolume' / 'segmentations.json') as f:\n",
    "            cloud_paths = json.load(f)\n",
    "    if 'Dynamic' in segmentation_version:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'],agglomerate=True,use_https=True,secrets=token)\n",
    "    else:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'])\n",
    "        \n",
    "    seg_ids = seg_from_pt(annotation_table.pt_position,cv)\n",
    "    \n",
    "    soma_table.name = annotation_table.tag\n",
    "    soma_table.pt_position = annotation_table.pt_position\n",
    "    soma_table.pt_root_id = seg_ids\n",
    "    soma_table.soma_x_nm = np.array([i[0] for i in annotation_table.pt_position]) * resolution[0]\n",
    "    soma_table.soma_y_nm = np.array([i[1] for i in annotation_table.pt_position]) * resolution[1]\n",
    "    soma_table.soma_z_nm = np.array([i[2] for i in annotation_table.pt_position]) * resolution[2]\n",
    "    \n",
    "    return(soma_table)\n",
    "\n",
    "\n",
    "\n",
    "def generate_synapse_table(annotation_table,\n",
    "                        segmentation_version='Dynamic_V1',\n",
    "                        resolution=np.array([4.3,4.3,45]),\n",
    "                        token=None):\n",
    "    ''' Generate a soma table used for microns analysis. This is the workaround for a materialization engine\n",
    "    Args:\n",
    "        annotation_table: pd.DataFrame, output from download_cell_table. Retreived from the annotation engine.\n",
    "        segmentation_version: str, Currently we have 4 for FANC. Two flat segmentations (\"Flat_1\" and \"Flat_2\") and two dynamic (\"Dynamic_V1/V2\"). \n",
    "                              This will only work if you have a segmentations.json in your cloudvolume folder. See examples for format.\n",
    "        resolution: np.array, Resolution of the mip0 coordinates of the version (not necessarily the same as the segmentation layer resolution).\n",
    "                              For all but the original FANC segmentation, this will be [4.3,4.3,45]\n",
    "        token: str, currently, CloudVolume requires a workaround for passing google secret tokens. This won't work unless you edit your cloudvolume \n",
    "                              file to remove the check for hexidecimal formatting of tokens. Updates should be coming to fix this. \n",
    "        '''\n",
    "     \n",
    "    \n",
    "    synapse_table = pd.DataFrame(columns=['id','pre_root_id','post_root_id',\n",
    "                                      'cleft_vx','ctr_pt_x_nm','ctr_pt_y_nm','ctr_pt_z_nm',\n",
    "                                      'pre_pos_x_vx','pre_pos_y_vx','pre_pos_z_vx',\n",
    "                                      'ctr_pos_x_vx','ctr_pos_y_vx','ctr_pos_z_vx',\n",
    "                                      'post_pos_x_vx','post_pos_y_vx','post_pos_z_vx'])\n",
    "\n",
    "    with open(Path.home() / 'cloudvolume' / 'segmentations.json') as f:\n",
    "            cloud_paths = json.load(f)\n",
    "    if 'Dynamic' in segmentation_version:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'],agglomerate=True,use_https=True,secrets=token)\n",
    "    else:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'])\n",
    "        \n",
    "    pre_ids = seg_from_pt(annotation_table.pre_pt_position,cv)\n",
    "    post_ids = seg_from_pt(annotation_table.post_pt_position,cv)\n",
    "    \n",
    "    synapse_table.pre_root_id = pre_ids\n",
    "    synapse_table.post_root_id = post_ids\n",
    "    \n",
    "    # TODO: This in not a stupid way. \n",
    "    synapse_table.ctr_pt_x_nm = np.array([i[0] for i in annotation_table.ctr_pt_position]) * resolution[0]\n",
    "    synapse_table.ctr_pt_y_nm = np.array([i[1] for i in annotation_table.ctr_pt_position]) * resolution[1]\n",
    "    synapse_table.ctr_pt_z_nm = np.array([i[2] for i in annotation_table.ctr_pt_position]) * resolution[2]\n",
    "    \n",
    "    synapse_table.pre_pos_x_vx = np.array([i[0] for i in annotation_table.pre_pt_position]) \n",
    "    synapse_table.pre_pos_y_vx = np.array([i[1] for i in annotation_table.pre_pt_position]) \n",
    "    synapse_table.pre_pos_z_vx = np.array([i[2] for i in annotation_table.pre_pt_position]) \n",
    "    \n",
    "    synapse_table.post_pos_x_vx = np.array([i[0] for i in annotation_table.post_pt_position]) \n",
    "    synapse_table.post_pos_x_vx = np.array([i[1] for i in annotation_table.post_pt_position]) \n",
    "    synapse_table.post_pos_x_vx = np.array([i[2] for i in annotation_table.post_pt_position]) \n",
    "    \n",
    "    return(synapse_table)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download tables\n",
    "- This will download tables from the annotation engine to then look up segment IDs and format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_table', 'test_synapse_table', 'test_synapse_table_2', 'test_T1MN_soma_table', 'T1MN_somas', '10B_somas', 'claw_neurons', '81A07_Synapses']\n"
     ]
    }
   ],
   "source": [
    "print(client.annotation.get_tables())\n",
    "#cell_table = download_annotation_table('test_T1MN_soma_table')\n",
    "syn_table = download_annotation_table('81A07_Synapses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctr_pt_position</th>\n",
       "      <th>post_pt_position</th>\n",
       "      <th>valid</th>\n",
       "      <th>size</th>\n",
       "      <th>pre_pt_position</th>\n",
       "      <th>deleted</th>\n",
       "      <th>created</th>\n",
       "      <th>id</th>\n",
       "      <th>superceded_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[48330, 111754, 2246]</td>\n",
       "      <td>[48371, 111742, 2246]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[48289, 111766, 2246]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-20 23:23:11.827602</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[48389, 111649, 2246]</td>\n",
       "      <td>[48394, 111682, 2246]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[48384, 111615, 2246]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-20 23:23:12.274774</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[47941, 111227, 2224]</td>\n",
       "      <td>[47962, 111216, 2224]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[47919, 111238, 2225]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-20 23:23:12.393396</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[47986, 110981, 2234]</td>\n",
       "      <td>[47961, 110950, 2234]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[48012, 111012, 2234]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-20 23:23:12.504269</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[48040, 110549, 2238]</td>\n",
       "      <td>[48088, 110528, 2238]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>[47992, 110571, 2238]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-20 23:23:12.614766</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ctr_pt_position       post_pt_position  valid  size  \\\n",
       "0  [48330, 111754, 2246]  [48371, 111742, 2246]   True  None   \n",
       "1  [48389, 111649, 2246]  [48394, 111682, 2246]   True  None   \n",
       "2  [47941, 111227, 2224]  [47962, 111216, 2224]   True  None   \n",
       "3  [47986, 110981, 2234]  [47961, 110950, 2234]   True  None   \n",
       "4  [48040, 110549, 2238]  [48088, 110528, 2238]   True  None   \n",
       "\n",
       "         pre_pt_position deleted                     created  id superceded_id  \n",
       "0  [48289, 111766, 2246]    None  2021-01-20 23:23:11.827602   1          None  \n",
       "1  [48384, 111615, 2246]    None  2021-01-20 23:23:12.274774   2          None  \n",
       "2  [47919, 111238, 2225]    None  2021-01-20 23:23:12.393396   3          None  \n",
       "3  [48012, 111012, 2234]    None  2021-01-20 23:23:12.504269   4          None  \n",
       "4  [47992, 110571, 2238]    None  2021-01-20 23:23:12.614766   5          None  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert annotation tables materialized(?) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.63it/s]A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "#soma_table = generate_soma_table(cell_table,token=dev_token)\n",
    "synapse_table = generate_synapse_table(syn_table,token=dev_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pre_root_id</th>\n",
       "      <th>post_root_id</th>\n",
       "      <th>cleft_vx</th>\n",
       "      <th>ctr_pt_x_nm</th>\n",
       "      <th>ctr_pt_y_nm</th>\n",
       "      <th>ctr_pt_z_nm</th>\n",
       "      <th>pre_pos_x_vx</th>\n",
       "      <th>pre_pos_y_vx</th>\n",
       "      <th>pre_pos_z_vx</th>\n",
       "      <th>ctr_pos_x_vx</th>\n",
       "      <th>ctr_pos_y_vx</th>\n",
       "      <th>ctr_pos_z_vx</th>\n",
       "      <th>post_pos_x_vx</th>\n",
       "      <th>post_pos_y_vx</th>\n",
       "      <th>post_pos_z_vx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>648518346720657339</td>\n",
       "      <td>648518346702967118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>207819.0</td>\n",
       "      <td>480542.2</td>\n",
       "      <td>101070.0</td>\n",
       "      <td>48289</td>\n",
       "      <td>111766</td>\n",
       "      <td>2246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>648518346707796904</td>\n",
       "      <td>648518346702967118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>208072.7</td>\n",
       "      <td>480090.7</td>\n",
       "      <td>101070.0</td>\n",
       "      <td>48384</td>\n",
       "      <td>111615</td>\n",
       "      <td>2246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>648518346720657339</td>\n",
       "      <td>648518346702967118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206146.3</td>\n",
       "      <td>478276.1</td>\n",
       "      <td>100080.0</td>\n",
       "      <td>47919</td>\n",
       "      <td>111238</td>\n",
       "      <td>2225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>648518346720657339</td>\n",
       "      <td>648518346702967118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206339.8</td>\n",
       "      <td>477218.3</td>\n",
       "      <td>100530.0</td>\n",
       "      <td>48012</td>\n",
       "      <td>111012</td>\n",
       "      <td>2234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>648518346694319648</td>\n",
       "      <td>648518346702967118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206572.0</td>\n",
       "      <td>475360.7</td>\n",
       "      <td>100710.0</td>\n",
       "      <td>47992</td>\n",
       "      <td>110571</td>\n",
       "      <td>2238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id         pre_root_id        post_root_id cleft_vx  ctr_pt_x_nm  \\\n",
       "0  NaN  648518346720657339  648518346702967118      NaN     207819.0   \n",
       "1  NaN  648518346707796904  648518346702967118      NaN     208072.7   \n",
       "2  NaN  648518346720657339  648518346702967118      NaN     206146.3   \n",
       "3  NaN  648518346720657339  648518346702967118      NaN     206339.8   \n",
       "4  NaN  648518346694319648  648518346702967118      NaN     206572.0   \n",
       "\n",
       "   ctr_pt_y_nm  ctr_pt_z_nm  pre_pos_x_vx  pre_pos_y_vx  pre_pos_z_vx  \\\n",
       "0     480542.2     101070.0         48289        111766          2246   \n",
       "1     480090.7     101070.0         48384        111615          2246   \n",
       "2     478276.1     100080.0         47919        111238          2225   \n",
       "3     477218.3     100530.0         48012        111012          2234   \n",
       "4     475360.7     100710.0         47992        110571          2238   \n",
       "\n",
       "  ctr_pos_x_vx ctr_pos_y_vx ctr_pos_z_vx  post_pos_x_vx post_pos_y_vx  \\\n",
       "0          NaN          NaN          NaN           2246           NaN   \n",
       "1          NaN          NaN          NaN           2246           NaN   \n",
       "2          NaN          NaN          NaN           2224           NaN   \n",
       "3          NaN          NaN          NaN           2234           NaN   \n",
       "4          NaN          NaN          NaN           2238           NaN   \n",
       "\n",
       "  post_pos_z_vx  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapse_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648518346720657339    6\n",
       "648518346726126149    3\n",
       "648518346723152571    2\n",
       "648518346682032599    2\n",
       "648518346707796904    2\n",
       "648518346700512319    1\n",
       "648518346662514761    1\n",
       "648518346732023811    1\n",
       "648518346702102374    1\n",
       "648518346672738793    1\n",
       "648518346657753929    1\n",
       "648518346657757769    1\n",
       "648518346658089033    1\n",
       "648518346677659113    1\n",
       "648518346702967118    1\n",
       "648518346716002138    1\n",
       "648518346705652344    1\n",
       "648518346716031578    1\n",
       "648518346694319648    1\n",
       "Name: pre_root_id, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapse_table['pre_root_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectomics_analysis",
   "language": "python",
   "name": "connectomics_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
