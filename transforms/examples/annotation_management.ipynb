{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cloudvolume import CloudVolume\n",
    "import json\n",
    "from annotationframeworkclient import FrameworkClient\n",
    "import nglui\n",
    "from concurrent import futures\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to the annotation engine:\n",
    "#### 1. Get a JSON state\n",
    "#### 2. Upload neurons or synapses\n",
    "---\n",
    "##### Token for V1 chunkedgraph on dev: https://dev.zetta.ai/wclee/auth/api/v1/refresh_token\n",
    "##### Token for annotation framework from here: https://api.zetta.ai/wclee/auth/api/v1/refresh_token\n",
    "- These tokens are currently stored in ~/cloudvolume/secrets/authtokens.json, you should put them here too since this folder will be used by other things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path.home() / 'cloudvolume' / 'secrets'/'chunkedgraph-secret.json') as f:\n",
    "        tokens = json.load(f)\n",
    "\n",
    "# Tokens from above links passed as strings. \n",
    "dev_token = tokens['dev']\n",
    "auth_token = tokens['api']\n",
    "\n",
    "datastack_name = 'vnc_v0' # from https://api.zetta.ai/wclee/info/\n",
    "\n",
    "client = FrameworkClient(\n",
    "    datastack_name,\n",
    "    server_address = \"https://api.zetta.ai/wclee\",\n",
    "    auth_token = auth_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the JSON state from a NG instance containing an annotation layer.\n",
    "- JSON states are the primary means of managing things.\n",
    "- When working in neuroglancer, to save the state of your workspace, press ctrl-shift-j. This will change the url with the /xxxxx being the JSON state ID. These states are stored in the JSON state service of the annotation framework. They are NOT EASY to look up, so here is a method for managing them.\n",
    "- Define a state manager to keep track of our states. Save the state dataframe to the ~/.cloudvolume. We will put this in a module later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateManager: \n",
    "    \n",
    "    ''' Class for keeping track of JSON states.'''\n",
    "    def __init__(self,\n",
    "                 filename=None,\n",
    "                 token=None):\n",
    "        \n",
    "        self.directory = Path.home() / 'cloudvolume'\n",
    "        if filename is None:\n",
    "            self.filename = self.directory / 'json_states.csv'\n",
    "        \n",
    "        self.__initialize()\n",
    "        \n",
    "        \n",
    "    def __initialize(self):\n",
    "        # Check if the database exists, if not create a new one.\n",
    "        fileEmpty =  os.path.exists(self.filename)\n",
    "        if not fileEmpty:\n",
    "            df = pd.DataFrame(columns=['state_id','description'])\n",
    "            df.to_csv(self.filename,index=False)\n",
    "        self.get_database()\n",
    "        print(self.df) \n",
    "    \n",
    "    def get_database(self):\n",
    "        # Read database. \n",
    "        self.df  = pd.read_csv(self.filename)\n",
    "        \n",
    " \n",
    "    def add_state(self, state_id, description=None):\n",
    "        \n",
    "        filename = self.filename\n",
    "        df = pd.DataFrame([{'state_id':state_id,'description':description}])\n",
    "        df.to_csv(filename, mode='a', header=False,index=False, encoding = 'utf-8')\n",
    "        self.get_database()\n",
    "        return('state added')\n",
    " \n",
    "    def get_state(self,state_id):\n",
    "        return(client.state.get_state_json(state_id))\n",
    "    \n",
    "    def get_url(self,state):\n",
    "        return('https://neuromancer-seung-import.appspot.com/?json_url=https://api.zetta.ai/json/' + str(state))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the state ID you want, or add a state with `sm.add_state()`\n",
    "#### Next, get the JSON state with `sm.get_state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             state_id     description\n",
      "0  261002187096550146  MNs,10Bs,Claws\n"
     ]
    }
   ],
   "source": [
    "sm = StateManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sm.get_state(sm.df.state_id[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at available layers\n",
    "- If you had annotation layers in your state, marking either synapses or cells, they will be shown here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['realigned_v1', 'vnc1_full_v3align_2', 'MNs', '10B', 'claws']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nglui.parser.layer_names(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some methods for formatting schemas. \n",
    "##### We are going to use two schemas for now. \n",
    "1. The synapse schema for synapses.\n",
    "    - To generate synapse entries, use an annotation layer that used line annotations with pt1 being the presynapse and pt2 being the postsynspse. \n",
    "2. The bound_tag schema for marking cells. \n",
    "    - To generate cell entreis, use an annotation layer that used point annotations, preferably on the soma (but not in the nucleus unless you merged the nucleus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soma_table_entries(state,layer_name = 'cells'):\n",
    "    ''' Generate entries for a soma table using the bound_tag schema \n",
    "    Args:\n",
    "    state: json, json state from get_json_state\n",
    "    layer_name: name of layer containing soma coords\n",
    "    ### Change this to points later. This is just for database copying because I used a stupid annotation\n",
    "    \n",
    "    Returns:\n",
    "    entries: list, list of dict entries for the bound_tag schema'''\n",
    "    \n",
    "    cell_layer = nglui.parser.get_layer(state,layer_name)\n",
    "    entries = []\n",
    "    for i in cell_layer['annotations']:\n",
    "        entry = {'tag': i['description'],\n",
    "                 'pt': {'position': i['pointA']}}\n",
    "        entries.append(entry)\n",
    "    return(entries)\n",
    "        \n",
    "\n",
    "def upload_cells(cell_entries,table_name,description=None):\n",
    "    ''' Upload cell entries to a soma dable using the bound_tag schema'\n",
    "    Args:\n",
    "    cell_entries: list, list of dicts from soma_table_entries\n",
    "    table_name: str, table name to create. If it exists, it will populate the existing one. \n",
    "    description: str, description to supply if creating a new table. It will fail if you don't provide one. \n",
    "    \n",
    "    ##TODO: Some sort of check for redundant synapses is necessary. We could make each upload need to go to a new table, but that seems absurd. \n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        client.annotation.create_table(table_name=table_name,\n",
    "                                   schema_name='bound_tag',\n",
    "                                   description=description)\n",
    "    except:\n",
    "        print('table exists')\n",
    "        \n",
    "    for i in cell_entries:\n",
    "        try:\n",
    "            client.annotation.post_annotation(table_name=table_name, data=[i])\n",
    "        except:\n",
    "            print('Fail',entry)\n",
    "\n",
    "\n",
    "\n",
    "def get_synapses(state,synapse_layer='synapses'):\n",
    "    ''' Get the synapse coordinates from a json state and return pre,post,center.\n",
    "    Args:\n",
    "        state: json,  json state from get_state_json \n",
    "        synapse_layer: str, name of layer containing synapses in the json state. Default is 'synapses'\n",
    "    Returns:\n",
    "        pre_pt,post_pt,ctr_pt: list, lists of coordinates for synapses.'''\n",
    "    \n",
    "    \n",
    "    syns = nglui.parser.line_annotations(state,synapse_layer)\n",
    "    if np.shape(syns)[0] != 2:\n",
    "        raise Exception( print('Incorrectly formatted synapse annotation. Requires two lists: Presynapse coordinates, Postsynapse coordinates'))\n",
    "           \n",
    "    else:\n",
    "        pre_pt = syns[0]\n",
    "        post_pt = syns[1]\n",
    "        ctr_pt = (np.array(pre_pt) + np.array(post_pt)) / 2\n",
    "    return(pre_pt,post_pt,np.ndarray.tolist(ctr_pt))\n",
    "\n",
    "\n",
    "def format_synapse(pre,post,ctr):\n",
    "    ''' Format a synapse for upload to a synapse table\n",
    "    Args:\n",
    "           pre: list, mip0 xyz coordinates to presynapse\n",
    "           post: list, mip0 xyz coordinates to postsynapse\n",
    "           ctr: list, mip0 xyz coordinates to center point\n",
    "    Returns:\n",
    "           data: formatted dict for a synapse table annotation upload\n",
    "    '''\n",
    "    data = {\n",
    "    \"type\": \"synapse\",\n",
    "    'pre_pt': {'position': pre},\n",
    "    'post_pt': {'position': post},\n",
    "    'ctr_pt': {'position': ctr}\n",
    "\n",
    "}\n",
    "    return(data)\n",
    "\n",
    "def upload_synapses(synapse_coordinates,table_name,description=None):\n",
    "    ''' Add synapses to a synapse table.\n",
    "    Args: \n",
    "        synapse_coordinates: list, list of shape [3,n,3]. Dim 0 is [pre,post,ctr], Dim 1 is the entry, Dim 2 is [x,y,z]. Output of get_synapses\n",
    "    ##TODO: Some sort of check for redundant synapses is necessary. We could make each upload need to go to a new table, but that seems absurd. \n",
    "    '''\n",
    "    try:\n",
    "        client.annotation.create_table(table_name=table_name,\n",
    "                                   schema_name='synapse',\n",
    "                                   description=description)\n",
    "    except:\n",
    "        print('table exists')\n",
    "        \n",
    "    for i in range(np.shape(synapse_coordinates)[1]):\n",
    "        try:\n",
    "            entry = format_synapse(synapse_coordinates[0][i],synapse_coordinates[1][i],synapse_coordinates[2][i])\n",
    "            client.annotation.post_annotation(table_name=table_name, data=[entry])\n",
    "        except:\n",
    "            print('Fail',entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a table from an annotation layer\n",
    "##### There currently are not checks on uploading duplicate data, so do not go crazy with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ef5a37b5a44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoma_table_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mupload_cells\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-e3e6fdd4e00c>\u001b[0m in \u001b[0;36msoma_table_entries\u001b[0;34m(state, layer_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     entries: list, list of dict entries for the bound_tag schema'''\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcell_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnglui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcell_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/parser/base.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(state, layer_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mLayer\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mlayer_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None is not in list"
     ]
    }
   ],
   "source": [
    "layer_name = None\n",
    "table_name = None\n",
    "description = None\n",
    "\n",
    "entries = soma_table_entries(state,layer_name=layer_name)\n",
    "upload_cells(entries,table_name,description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some methods for working around the materialization engine.\n",
    "    1. Download an annotation table\n",
    "    2. Look up the root_ids associated with the points\n",
    "    3. Generate a dataframe that looks like the microns format (post materialization?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_annotation_table(table_name,ids=range(1000)):\n",
    "    entries = client.annotation.get_annotation(table_name,ids)\n",
    "    annotation_table = pd.DataFrame(entries)\n",
    "    return(annotation_table)\n",
    "\n",
    "\n",
    "def seg_from_pt(pts,vol,image_res=np.array([4.3,4.3,45]),max_workers=4):\n",
    "    ''' Get segment ID at a point. Default volume is the static segmentation layer for now. \n",
    "    Args:\n",
    "        pts (list): list of 3-element np.arrays of MIP0 coordinates\n",
    "        vol_url (str): cloud volume url\n",
    "    Returns:\n",
    "        list, segment_ID at specified point '''\n",
    "    \n",
    "    \n",
    "    seg_mip = vol.scale['resolution']\n",
    "    res = seg_mip / image_res\n",
    "\n",
    "    pts_scaled = [pt // res for pt in pts]\n",
    "    results = []\n",
    "    with futures.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        point_futures = [ex.submit(lambda pt,vol: vol[list(pt)][0][0][0][0], k,vol) for k in pts_scaled]\n",
    "        \n",
    "        for f in futures.as_completed(point_futures):\n",
    "            results=[f.result() for f in point_futures]\n",
    "       \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def generate_soma_table(annotation_table,\n",
    "                        segmentation_version='Dynamic_V1',\n",
    "                        resolution=np.array([4.3,4.3,45]),\n",
    "                        token=None):\n",
    "    ''' Generate a soma table used for microns analysis. This is the workaround for a materialization engine\n",
    "    Args:\n",
    "        annotation_table: pd.DataFrame, output from download_cell_table. Retreived from the annotation engine.\n",
    "        segmentation_version: str, Currently we have 4 for FANC. Two flat segmentations (\"Flat_1\" and \"Flat_2\") and two dynamic (\"Dynamic_V1/V2\"). \n",
    "                              This will only work if you have a segmentations.json in your cloudvolume folder. See examples for format.\n",
    "        resolution: np.array, Resolution of the mip0 coordinates of the version (not necessarily the same as the segmentation layer resolution).\n",
    "                              For all but the original FANC segmentation, this will be [4.3,4.3,45]\n",
    "        token: str, currently, CloudVolume requires a workaround for passing google secret tokens. This won't work unless you edit your cloudvolume \n",
    "                              file to remove the check for hexidecimal formatting of tokens. Updates should be coming to fix this. \n",
    "        '''\n",
    "\n",
    "    soma_table = pd.DataFrame(columns=['name','cell_type',\n",
    "                                       'pt_position','pt_root_id',\n",
    "                                       'soma_x_nm','soma_y_nm','soma_z_nm',\n",
    "                                       'found'])\n",
    "    with open(Path.home() / 'cloudvolume' / 'segmentations.json') as f:\n",
    "            cloud_paths = json.load(f)\n",
    "    if 'Dynamic' in segmentation_version:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'],agglomerate=True,use_https=True,secrets=token)\n",
    "    else:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'])\n",
    "        \n",
    "    seg_ids = seg_from_pt(annotation_table.pt_position,cv)\n",
    "    \n",
    "    soma_table.name = annotation_table.tag\n",
    "    soma_table.pt_position = annotation_table.pt_position\n",
    "    soma_table.pt_root_id = seg_ids\n",
    "    soma_table.soma_x_nm = np.array([i[0] for i in annotation_table.pt_position]) * resolution[0]\n",
    "    soma_table.soma_y_nm = np.array([i[1] for i in annotation_table.pt_position]) * resolution[1]\n",
    "    soma_table.soma_z_nm = np.array([i[2] for i in annotation_table.pt_position]) * resolution[2]\n",
    "    \n",
    "    return(soma_table)\n",
    "\n",
    "\n",
    "\n",
    "def generate_synapse_table(annotation_table,\n",
    "                        segmentation_version='Dynamic_V1',\n",
    "                        resolution=np.array([4.3,4.3,45]),\n",
    "                        token=None):\n",
    "    ''' Generate a soma table used for microns analysis. This is the workaround for a materialization engine\n",
    "    Args:\n",
    "        annotation_table: pd.DataFrame, output from download_cell_table. Retreived from the annotation engine.\n",
    "        segmentation_version: str, Currently we have 4 for FANC. Two flat segmentations (\"Flat_1\" and \"Flat_2\") and two dynamic (\"Dynamic_V1/V2\"). \n",
    "                              This will only work if you have a segmentations.json in your cloudvolume folder. See examples for format.\n",
    "        resolution: np.array, Resolution of the mip0 coordinates of the version (not necessarily the same as the segmentation layer resolution).\n",
    "                              For all but the original FANC segmentation, this will be [4.3,4.3,45]\n",
    "        token: str, currently, CloudVolume requires a workaround for passing google secret tokens. This won't work unless you edit your cloudvolume \n",
    "                              file to remove the check for hexidecimal formatting of tokens. Updates should be coming to fix this. \n",
    "        '''\n",
    "     \n",
    "    \n",
    "    synapse_table = pd.DataFrame(columns=['id','pre_root_id','post_root_id',\n",
    "                                      'cleft_vx','ctr_pt_x_nm','ctr_pt_y_nm','ctr_pt_z_nm',\n",
    "                                      'pre_pos_x_vx','pre_pos_y_vx','pre_pos_z_vx',\n",
    "                                      'ctr_pos_x_vx','ctr_pos_y_vx','ctr_pos_z_vx',\n",
    "                                      'post_pos_x_vx','post_pos_y_vx','post_pos_z_vx'])\n",
    "\n",
    "    with open(Path.home() / 'cloudvolume' / 'segmentations.json') as f:\n",
    "            cloud_paths = json.load(f)\n",
    "    if 'Dynamic' in segmentation_version:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'],agglomerate=True,use_https=True,secrets=token)\n",
    "    else:\n",
    "        cv = CloudVolume(cloud_paths[segmentation_version]['url'])\n",
    "        \n",
    "    pre_ids = seg_from_pt(annotation_table.pre_pt_position,cv)\n",
    "    post_ids = seg_from_pt(annotation_table.post_pt_position,cv)\n",
    "    \n",
    "    synapse_table.pre_root_id = pre_ids\n",
    "    synapse_table.post_root_id = post_ids\n",
    "    \n",
    "    # TODO: This in not a stupid way. \n",
    "    synapse_table.ctr_pt_x_nm = np.array([i[0] for i in annotation_table.ctr_pt_position]) * resolution[0]\n",
    "    synapse_table.ctr_pt_y_nm = np.array([i[1] for i in annotation_table.ctr_pt_position]) * resolution[1]\n",
    "    synapse_table.ctr_pt_z_nm = np.array([i[2] for i in annotation_table.ctr_pt_position]) * resolution[2]\n",
    "    \n",
    "    synapse_table.pre_pos_x_vx = np.array([i[0] for i in annotation_table.pre_pt_position]) \n",
    "    synapse_table.pre_pos_y_vx = np.array([i[1] for i in annotation_table.pre_pt_position]) \n",
    "    synapse_table.pre_pos_z_vx = np.array([i[2] for i in annotation_table.pre_pt_position]) \n",
    "    \n",
    "    synapse_table.post_pos_x_vx = np.array([i[0] for i in annotation_table.post_pt_position]) \n",
    "    synapse_table.post_pos_x_vx = np.array([i[1] for i in annotation_table.post_pt_position]) \n",
    "    synapse_table.post_pos_x_vx = np.array([i[2] for i in annotation_table.post_pt_position]) \n",
    "    \n",
    "    return(synapse_table)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download tables\n",
    "- This will download tables from the annotation engine to then look up segment IDs and format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_table', 'test_synapse_table', 'test_synapse_table_2', 'test_T1MN_soma_table', 'T1MN_somas', '10B_somas', 'claw_neurons']\n"
     ]
    }
   ],
   "source": [
    "print(client.annotation.get_tables())\n",
    "cell_table = download_annotation_table('test_T1MN_soma_table')\n",
    "syn_table = download_annotation_table('test_synapse_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert annotation tables materialized(?) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.48it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.53it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.49it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\u001b[A\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\u001b[A\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s]A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  4.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\u001b[A\u001b[A\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "soma_table = generate_soma_table(cell_table,token=dev_token)\n",
    "synapse_table = generate_synapse_table(syn_table,token=dev_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>pt_position</th>\n",
       "      <th>pt_root_id</th>\n",
       "      <th>soma_x_nm</th>\n",
       "      <th>soma_y_nm</th>\n",
       "      <th>soma_z_nm</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MN_A101_T1L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[13329, 114635, 1861]</td>\n",
       "      <td>648518346704067890</td>\n",
       "      <td>57314.7</td>\n",
       "      <td>492930.5</td>\n",
       "      <td>83745.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MN_A101_T1R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[63394, 114393, 2304]</td>\n",
       "      <td>648518346662254153</td>\n",
       "      <td>272594.2</td>\n",
       "      <td>491889.9</td>\n",
       "      <td>103680.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MN_A102_T1L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[11729, 116735, 1478]</td>\n",
       "      <td>648518346682057412</td>\n",
       "      <td>50434.7</td>\n",
       "      <td>501960.5</td>\n",
       "      <td>66510.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MN_A102_T1R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[63084, 116459, 2863]</td>\n",
       "      <td>648518346676423055</td>\n",
       "      <td>271261.2</td>\n",
       "      <td>500773.7</td>\n",
       "      <td>128835.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MN_A103_T1R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[59659, 115855, 2065]</td>\n",
       "      <td>648518346702534824</td>\n",
       "      <td>256533.7</td>\n",
       "      <td>498176.5</td>\n",
       "      <td>92925.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>MN_V402_T1L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[23167, 87333, 2184]</td>\n",
       "      <td>648518346680460455</td>\n",
       "      <td>99618.1</td>\n",
       "      <td>375531.9</td>\n",
       "      <td>98280.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>MN_V501_T1L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[27494, 86790, 2515]</td>\n",
       "      <td>648518346661954522</td>\n",
       "      <td>118224.2</td>\n",
       "      <td>373197.0</td>\n",
       "      <td>113175.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>MN_V501_T1R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[54883, 84397, 2319]</td>\n",
       "      <td>648518346666755657</td>\n",
       "      <td>235996.9</td>\n",
       "      <td>362907.1</td>\n",
       "      <td>104355.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>MN_V601_T1R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[49727, 85190, 2264]</td>\n",
       "      <td>648518346662417225</td>\n",
       "      <td>213826.1</td>\n",
       "      <td>366317.0</td>\n",
       "      <td>101880.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>MN_V601_T1L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[21122, 88740, 2060]</td>\n",
       "      <td>648518346705861496</td>\n",
       "      <td>90824.6</td>\n",
       "      <td>381582.0</td>\n",
       "      <td>92700.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name cell_type            pt_position          pt_root_id  \\\n",
       "0    MN_A101_T1L       NaN  [13329, 114635, 1861]  648518346704067890   \n",
       "1    MN_A101_T1R       NaN  [63394, 114393, 2304]  648518346662254153   \n",
       "2    MN_A102_T1L       NaN  [11729, 116735, 1478]  648518346682057412   \n",
       "3    MN_A102_T1R       NaN  [63084, 116459, 2863]  648518346676423055   \n",
       "4    MN_A103_T1R       NaN  [59659, 115855, 2065]  648518346702534824   \n",
       "..           ...       ...                    ...                 ...   \n",
       "116  MN_V402_T1L       NaN   [23167, 87333, 2184]  648518346680460455   \n",
       "117  MN_V501_T1L       NaN   [27494, 86790, 2515]  648518346661954522   \n",
       "118  MN_V501_T1R       NaN   [54883, 84397, 2319]  648518346666755657   \n",
       "119  MN_V601_T1R       NaN   [49727, 85190, 2264]  648518346662417225   \n",
       "120  MN_V601_T1L       NaN   [21122, 88740, 2060]  648518346705861496   \n",
       "\n",
       "     soma_x_nm  soma_y_nm  soma_z_nm found  \n",
       "0      57314.7   492930.5    83745.0   NaN  \n",
       "1     272594.2   491889.9   103680.0   NaN  \n",
       "2      50434.7   501960.5    66510.0   NaN  \n",
       "3     271261.2   500773.7   128835.0   NaN  \n",
       "4     256533.7   498176.5    92925.0   NaN  \n",
       "..         ...        ...        ...   ...  \n",
       "116    99618.1   375531.9    98280.0   NaN  \n",
       "117   118224.2   373197.0   113175.0   NaN  \n",
       "118   235996.9   362907.1   104355.0   NaN  \n",
       "119   213826.1   366317.0   101880.0   NaN  \n",
       "120    90824.6   381582.0    92700.0   NaN  \n",
       "\n",
       "[121 rows x 8 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soma_table.to_csv('/Users/brandon/Documents/Repositories/Python/Neuron_Database/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectomics_analysis",
   "language": "python",
   "name": "connectomics_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
