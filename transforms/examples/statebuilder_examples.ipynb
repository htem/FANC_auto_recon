{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted from : https://github.com/seung-lab/NeuroglancerAnnotationUI/tree/master/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Basics\n",
    "\n",
    "The Statebuilder is a submodule that helps produce custom Neuroglancer states based on data in the form of Pandas dataframes. Neuroglancer organizes data sources into layers. Layers come in three types, image, segmentation, and annotation. Each has different properties and functions. The state builder lets the user define a set of rules for initializing layers and how to map data columns to selections and annotations. Let's see the simplest example in use now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nglui.statebuilder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Layers\n",
    "\n",
    "Image layers in Neuroglancer are pretty simple. An image has a name (by default 'img' here) and a source, which is typically a path to a cloud hosted 3d image volume. We define an image layer with an ImageLayerConfig that lets the user set these key parameters. We are going to use the public Layer 2/3 EM dataset at [Microns Explorer](https://layer23.microns-explorer.org) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_source = 'precomputed://gs://zetta_lee_fly_vnc_001_precomputed/vnc1_full_v3align_2/realigned_v1'\n",
    "img_layer = ImageLayerConfig(name='img',\n",
    "                             source=img_source,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation Layers\n",
    "Segmentation layers in Neuroglancer are more complex, since each object has a unique id and can be selected, loading the object and its mesh into the neuroglancer state. Like images, a segmentation layer has a name and a source. Neuroglancer supports two types of segmentation volume, 'precomputed' and 'graphene'. Precomputed is what we call a \"flat\" segmentation, in that the segmentation is frozen into an unchanging state. Flat segmentations are fast, but cannot be edited to fix mistakes. Graphene segmentations are dynamic, using an efficient graph data representation to allow edits to the segmentation state. For the most part, users should not need to care about the difference.\n",
    "\n",
    "Segmentation layers are configured by a SegmentationLayerConfig. At a minimum, a SegmentionLayerConfig has the same setup as an ImageLayerConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_source = 'graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2'\n",
    "seg_layer = SegmentationLayerConfig(name = 'img',\n",
    "                                    source = seg_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation Layers\n",
    "\n",
    "Annotation layers let a user define various spatial data annotations. We can define annotation layers with AnnotationLayerConfig objects. While annotation layers have a name, they don't have a source. We'll discuss how to map data to annotations later, but for now we will add a blank annotation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_layer = AnnotationLayerConfig(name='img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StateBuilders and state rendering\n",
    "\n",
    "At it's most basic, the StateBuilder is initialized with layer configs for each layer the user wants. A state isn't actually generated until the user calls `render_state`. While the function can also take a dataframe, it works with no argument to generate a default state. The default output is a url string that specifies the neuroglancer state, however other options are available with the optional `return_as` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://neuromancer-seung-import.appspot.com/#!%7B%22navigation%22:%7B%22pose%22:%7B%22position%22:%7B%22voxelSize%22:%5B4.300000190734863,4.300000190734863,45.0%5D%7D%7D,%22zoomFactor%22:2.0%7D,%22showSlices%22:false,%22layout%22:%22xy-3d%22,%22perspectiveZoom%22:2000.0,%22layers%22:%5B%7B%22type%22:%22segmentation_with_graph%22,%22source%22:%22graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2%22,%22name%22:%22img%22%7D%5D,%22selectedLayer%22:%7B%22layer%22:%22img%22,%22visible%22:true%7D%7D'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = StateBuilder(layers=[img_layer, seg_layer, anno_layer],resolution=[4.3,4.3,45])\n",
    "sb.render_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `return_as=\"html\"` provides a link, useful for interactive notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://neuromancer-seung-import.appspot.com/#!%7B%22navigation%22:%7B%22pose%22:%7B%22position%22:%7B%22voxelSize%22:%5B4.300000190734863,4.300000190734863,45.0%5D%7D%7D,%22zoomFactor%22:2.0%7D,%22showSlices%22:false,%22layout%22:%22xy-3d%22,%22perspectiveZoom%22:2000.0,%22layers%22:%5B%7B%22type%22:%22segmentation_with_graph%22,%22source%22:%22graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2%22,%22name%22:%22img%22%7D%5D,%22selectedLayer%22:%7B%22layer%22:%22img%22,%22visible%22:true%7D%7D\" target=\"_blank\">Neuroglancer Link</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.render_state(return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sb.render_state(return_as='json')` returns the JSON state as a string. This can be pasted directly into the neuroglancer JSON state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"navigation\": {\"pose\": {\"position\": {\"voxelSize\": [4.300000190734863, 4.300000190734863, 45.0]}}, \"zoomFactor\": 2.0}, \"showSlices\": false, \"layout\": \"xy-3d\", \"perspectiveZoom\": 2000.0, \"layers\": [{\"type\": \"segmentation_with_graph\", \"source\": \"graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2\", \"name\": \"img\"}], \"selectedLayer\": {\"layer\": \"img\", \"visible\": true}}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.render_state(return_as='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sb.render_state(return_as='dict')` returns the state as a dictionary, useful for inspection and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('navigation',\n",
       "              OrderedDict([('pose',\n",
       "                            OrderedDict([('position',\n",
       "                                          OrderedDict([('voxelSize',\n",
       "                                                        [4.300000190734863,\n",
       "                                                         4.300000190734863,\n",
       "                                                         45.0])]))])),\n",
       "                           ('zoomFactor', 2.0)])),\n",
       "             ('showSlices', False),\n",
       "             ('layout', 'xy-3d'),\n",
       "             ('perspectiveZoom', 2000.0),\n",
       "             ('layers',\n",
       "              [OrderedDict([('type', 'segmentation_with_graph'),\n",
       "                            ('source',\n",
       "                             'graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2'),\n",
       "                            ('name', 'img')])]),\n",
       "             ('selectedLayer',\n",
       "              OrderedDict([('layer', 'img'), ('visible', True)]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.render_state(return_as='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using `sb.render_state(return_as='viewer')` returns an EasyViewer object, which can be further manipulated (see documentation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViewerState({\"navigation\": {\"pose\": {\"position\": {\"voxelSize\": [4.300000190734863, 4.300000190734863, 45.0]}}, \"zoomFactor\": 2.0}, \"showSlices\": false, \"layout\": \"xy-3d\", \"perspectiveZoom\": 2000.0, \"layers\": [{\"type\": \"segmentation_with_graph\", \"source\": \"graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2\", \"name\": \"img\"}], \"selectedLayer\": {\"layer\": \"img\", \"visible\": true}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vwr = sb.render_state(return_as='viewer')\n",
    "vwr.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data-responsive state generation\n",
    "\n",
    "The key feature of the StateBuilder is to use data to add selected objects or annotations to a Neuroglancer state. Each layer has rules for how to map dataframe columns to its state. This is designed to be useful for consistent mapping of queries or analysis directly into data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected segmentations\n",
    "\n",
    "Segmentation layers principally control what what objects are selected. Objects are specified by root id and `SegmentationLayerConfig` can hold rules for how to select data.\n",
    "\n",
    "The `soma_df` example dataframe describes all the named T1 Motor neurons. These dataframes can be generated by downloading cell tables from the annotation engine. Each row is a different cell and it is described by a number of columns, of which 'pt_root_id' has the root id for each motor neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "soma_df = pd.read_csv('/Users/brandon/Documents/Repositories/Python/Neuron_Database/T1MN_somas.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common task is to all the ids in a column of the dataframe, perhaps as a result of a query or some bit of analysis. We can tell the `SegmentationLayerConfig` which layer (or list of layers) to use with the `selected_ids_column` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_layer = SegmentationLayerConfig(name = 'MNs',\n",
    "                                    source = seg_source,\n",
    "                                    selected_ids_column='pt_root_id')\n",
    "\n",
    "sb = StateBuilder(layers=[img_layer, seg_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our statebuilder object also needs data. The `render_data` method can always take a dataframe. To avoid selecting hundreds of neurons, let's just take the first five rows of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://neuromancer-seung-import.appspot.com/#!%7B%22navigation%22:%7B%22pose%22:%7B%22position%22:%7B%22voxelSize%22:%5B4.0,4.0,40.0%5D%7D%7D,%22zoomFactor%22:2.0%7D,%22showSlices%22:false,%22layout%22:%22xy-3d%22,%22perspectiveZoom%22:2000.0,%22layers%22:%5B%7B%22type%22:%22image%22,%22source%22:%22precomputed://gs://zetta_lee_fly_vnc_001_precomputed/vnc1_full_v3align_2/realigned_v1%22,%22name%22:%22img%22%7D,%7B%22type%22:%22segmentation_with_graph%22,%22source%22:%22graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2%22,%22name%22:%22MNs%22,%22segments%22:%5B%22648518346729150467%22,%22648518346726264323%22,%22648518346726249475%22,%22648518346701393983%22,%22648518346697519679%22,%22648518346701348927%22,%22648518346699540543%22,%22648518346695376963%22,%22648518346699524671%22,%22648518346697521727%22,%22648518346662254153%22,%22648518346662074953%22,%22648518346662363721%22,%22648518346662603337%22,%22648518346666755657%22,%22648518346711046234%22,%22648518346716451930%22,%22648518346710899802%22,%22648518346684560481%22,%22648518346689162849%22,%22648518346689120865%22,%22648518346689116769%22,%22648518346684653665%22,%22648518346696470630%22,%22648518346697120358%22,%22648518346697281126%22,%22648518346697200742%22,%22648518346700451942%22,%22648518346696638054%22,%22648518346696524390%22,%22648518346697603686%22,%22648518346696541798%22,%22648518346703664247%22,%22648518346700815480%22,%22648518346703985272%22,%22648518346700588152%22,%22648518346708311160%22,%22648518346702754424%22,%22648518346707740280%22,%22648518346673801369%22,%22648518346680460455%22,%22648518346702534824%22,%22648518346684196522%22,%22648518346683369642%22,%22648518346684192426%22,%22648518346666514616%22,%22648518346725338813%22,%22648518346682057412%22,%22648518346672938187%22,%22648518346684155095%22,%22648518346730375387%22,%22648518346693778142%22,%22648518346726213359%22,%22648518346682807026%22,%22648518346679668978%22,%22648518346681322738%22,%22648518346676222706%22,%22648518346681726194%22,%22648518346726558467%22,%22648518346726269187%22,%22648518346725597443%22,%22648518346670637324%22,%22648518346700843809%22,%22648518346661194538%22,%22648518346704067890%22,%22648518346699520319%22,%22648518346697839423%22,%22648518346699526463%22,%22648518346699540799%22,%22648518346725166405%22,%22648518346659995977%22,%22648518346660605769%22,%22648518346658396489%22,%22648518346662176585%22,%22648518346658525001%22,%22648518346662417225%22,%22648518346713855834%22,%22648518346711451994%22,%22648518346711022426%22,%22648518346696256870%22,%22648518346697111398%22,%22648518346696279398%22,%22648518346697321830%22,%22648518346696438630%22,%22648518346701537126%22,%22648518346696751984%22,%22648518346708317560%22,%22648518346702940024%22,%22648518346707880824%22,%22648518346707145080%22,%22648518346705861496%22,%22648518346676423055%22,%22648518346701817256%22,%22648518346699399593%22,%22648518346682395562%22,%22648518346684490154%22,%22648518346699427753%22,%22648518346682527146%22,%22648518346700768680%22,%22648518346666514360%22,%22648518346725378493%22,%22648518346672238027%22,%22648518346703683535%22,%22648518346680376791%22,%22648518346661954522%22,%22648518346736009179%22,%22648518346689973214%22,%22648518346696497118%22,%22648518346694368734%22,%22648518346690096094%22,%22648518346693246430%22,%22648518346725465583%22,%22648518346679646706%22,%22648518346680376818%22,%22648518346682508274%22,%22648518346679863282%22,%22648518346691199585%22%5D,%22segmentColors%22:%7B%7D%7D%5D%7D\" target=\"_blank\">Neuroglancer Link</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.render_state(soma_df, return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some purposes, it can also be useful to force certain objects to be selected no matter the data, for example if you want to show various points in space relative to a specific neuron. For that, we can use the `fixed_ids` argument. Here, we use it to load neuron `648518346349537042` no matter the data. Both data-driven and fixed ids can be used together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_layer = SegmentationLayerConfig(name = 'seg',\n",
    "                                    source = seg_source,\n",
    "                                    selected_ids_column='pt_root_id',\n",
    "                                    fixed_ids=[648518346349537042])\n",
    "\n",
    "sb = StateBuilder(layers=[img_layer, seg_layer, anno_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output with and without data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://neuromancer-seung-import.appspot.com/#!%7B%22navigation%22:%7B%22pose%22:%7B%22position%22:%7B%22voxelSize%22:%5B4.0,4.0,40.0%5D%7D%7D,%22zoomFactor%22:2.0%7D,%22showSlices%22:false,%22layout%22:%22xy-3d%22,%22perspectiveZoom%22:2000.0,%22layers%22:%5B%7B%22type%22:%22image%22,%22source%22:%22precomputed://gs://zetta_lee_fly_vnc_001_precomputed/vnc1_full_v3align_2/realigned_v1%22,%22name%22:%22img%22%7D,%7B%22type%22:%22segmentation_with_graph%22,%22source%22:%22graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2%22,%22name%22:%22seg%22,%22segments%22:%5B%22648518346349537042%22%5D,%22segmentColors%22:%7B%7D%7D%5D,%22selectedLayer%22:%7B%22layer%22:%22img%22,%22visible%22:true%7D%7D\" target=\"_blank\">State without data</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.render_state(return_as='html', link_text='State without data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://neuromancer-seung-import.appspot.com/#!%7B%22navigation%22:%7B%22pose%22:%7B%22position%22:%7B%22voxelSize%22:%5B4.0,4.0,40.0%5D%7D%7D,%22zoomFactor%22:2.0%7D,%22showSlices%22:false,%22layout%22:%22xy-3d%22,%22perspectiveZoom%22:2000.0,%22layers%22:%5B%7B%22type%22:%22image%22,%22source%22:%22precomputed://gs://zetta_lee_fly_vnc_001_precomputed/vnc1_full_v3align_2/realigned_v1%22,%22name%22:%22img%22%7D,%7B%22type%22:%22segmentation_with_graph%22,%22source%22:%22graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2%22,%22name%22:%22seg%22,%22segments%22:%5B%22648518346662254153%22,%22648518346704067890%22,%22648518346349537042%22%5D,%22segmentColors%22:%7B%7D%7D%5D,%22selectedLayer%22:%7B%22layer%22:%22img%22,%22visible%22:true%7D%7D\" target=\"_blank\">State with selection data</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.render_state(soma_df.head(2), return_as='html', link_text='State with selection data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning colors to cells\n",
    "\n",
    "You can also provide a column that species the color for each root id with the `color_column` argument. Colors in Neuroglancer are specified as an RGB hex string, for example a pure red would be `#ff0000`.\n",
    "\n",
    "In the example below, we specify the color list directly. Data visualization packages typically have methods to convert numeric RGB vectors to hex, for instance `matplotlib.colors.to_hex` for `matplotlib`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://neuromancer-seung-import.appspot.com/#!%7B%22navigation%22:%7B%22pose%22:%7B%22position%22:%7B%22voxelSize%22:%5B4.0,4.0,40.0%5D%7D%7D,%22zoomFactor%22:2.0%7D,%22showSlices%22:false,%22layout%22:%22xy-3d%22,%22perspectiveZoom%22:2000.0,%22layers%22:%5B%7B%22type%22:%22image%22,%22source%22:%22precomputed://gs://zetta_lee_fly_vnc_001_precomputed/vnc1_full_v3align_2/realigned_v1%22,%22name%22:%22img%22%7D,%7B%22type%22:%22segmentation_with_graph%22,%22source%22:%22graphene://https://dev.zetta.ai/wclee/segmentation/table/vnc1_full_v3align_2%22,%22name%22:%22seg%22,%22segments%22:%5B%22648518346682057412%22,%22648518346702534824%22,%22648518346662254153%22,%22648518346676423055%22,%22648518346704067890%22%5D,%22segmentColors%22:%7B%22648518346704067890%22:%22#fdd4c2%22,%22648518346662254153%22:%22#fca082%22,%22648518346682057412%22:%22#fb694a%22,%22648518346676423055%22:%22#e32f27%22,%22648518346702534824%22:%22#b11218%22%7D%7D%5D%7D\" target=\"_blank\">State with color</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we have to add a column with legitimate colors\n",
    "reds = ['#fdd4c2', '#fca082', '#fb694a', '#e32f27', '#b11218']\n",
    "soma_colored_df = soma_df.head(5).copy()\n",
    "soma_colored_df['color'] = reds\n",
    "\n",
    "# Next we specify the color column when defining the segmentation layer.\n",
    "seg_layer = SegmentationLayerConfig(name = 'seg', source=seg_source, selected_ids_column='pt_root_id', color_column='color')\n",
    "\n",
    "sb = StateBuilder(layers=[img_layer, seg_layer])\n",
    "sb.render_state(soma_colored_df, return_as='html', link_text='State with color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-driven annotations\n",
    "\n",
    "Neuroglancer offers Annotation layers, which can put different kinds of markers in the volume. There are three main marker types:\n",
    "* Points\n",
    "* Lines\n",
    "* Spheres\n",
    "\n",
    "Each annotation layer can hold any collection of types, however colors and other organizational properties are shared among all annotations in one layer. To make a new annotation layer, we use an `AnnotationLayerConfig`. Unlike segmentation and image layers, there is no data source, but the data mapping options are more rich. Each annotation type has a mapper class (PointMapper, LineMapper, SphereMapper) to designate the rules. Each AnnotationLayerConfig works for a single annotation layer, but can take an arbitrary number of mappers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point annotations\n",
    "Point annotations are a simple point in 3d space. Let's use the positions from the soma_df above. The only thing you need to set a point mapper is the column name, which should reference a column of 3-element locations in units of voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '[13329, 114635, 1861]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5f4db17b7b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStateBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manno_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoma_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_as\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/statebuilder/statebuilder.py\u001b[0m in \u001b[0;36mrender_state\u001b[0;34m(self, data, base_state, return_as, url_prefix, link_text)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0murl_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_as\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'viewer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/statebuilder/statebuilder.py\u001b[0m in \u001b[0;36m_render_layers\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_viewer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/statebuilder/statebuilder.py\u001b[0m in \u001b[0;36m_render_layer\u001b[0;34m(self, viewer, data)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \"\"\" Applies rendering rules\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_specific_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_selected_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/statebuilder/statebuilder.py\u001b[0m in \u001b[0;36m_specific_rendering\u001b[0;34m(self, viewer, data)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                     \u001b[0mannos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m                     \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_view_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/statebuilder/statebuilder.py\u001b[0m in \u001b[0;36m_render_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    581\u001b[0m                                              \u001b[0mlinked_segmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                                              tag_ids=t) for\n\u001b[0;32m--> 583\u001b[0;31m                  pt, d, ls, t in zip(pts, descriptions, linked_segs, tags)]\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_column\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/statebuilder/statebuilder.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    581\u001b[0m                                              \u001b[0mlinked_segmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                                              tag_ids=t) for\n\u001b[0;32m--> 583\u001b[0;31m                  pt, d, ls, t in zip(pts, descriptions, linked_segs, tags)]\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_column\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/easyviewer/annotation.py\u001b[0m in \u001b[0;36mpoint_annotation\u001b[0;34m(point, id, description, linked_segmentation, tag_ids)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_random_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     point = PointAnnotation(\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/connectomics_analysis/lib/python3.7/site-packages/nglui/easyviewer/annotation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_random_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     point = PointAnnotation(\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '[13329, 114635, 1861]'"
     ]
    }
   ],
   "source": [
    "points = PointMapper(point_column='pt_position')\n",
    "anno_layer = AnnotationLayerConfig(name='annos',\n",
    "                                   mapping_rules=points )\n",
    "\n",
    "\n",
    "# Make a basic segmentation source\n",
    "seg_layer = SegmentationLayerConfig(seg_source)\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer, anno_layer])\n",
    "sb.render_state(soma_df, return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line annotations\n",
    "\n",
    "Line differ from point annotations only by requiring two columns, not just one. The two columns set the beginning and end points of the line and are thus both columns should contain three element points. Let's use the example synapse dataframe as an example. We're going to use the `pre_pt_root_id` field to select the neuron ids to show and use the lines to indicate their outgoing synapses, which go from `ct_pt_position`, a point on the synaptic cleft itself, to `post_pt_position`, a point somewhat inside the target neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = LineMapper(point_column_a='ctr_pt_position', point_column_b='post_pt_position')\n",
    "anno_layer = AnnotationLayerConfig(name='synapses',\n",
    "                                   mapping_rules=lines)\n",
    "\n",
    "# Make a segmentation source with a selected ids column\n",
    "seg_layer = SegmentationLayerConfig(seg_source, selected_ids_column='pre_pt_root_id')\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer, anno_layer])\n",
    "sb.render_state(pre_syn_df, return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sphere annotations\n",
    "\n",
    "Like line annotations, sphere annotations take two columns. The first is the center point (and thus a point in space) while the second is the radius in voxels (x/y only), and thus numeric. We're going to use the soma positions for the example, using the `pt_position` for the centers. Since we don't have radius data in the frame, we will add a column with a random radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "soma_df['radius'] = np.random.normal(1500, 250, len(soma_df)).astype(int)\n",
    "\n",
    "\n",
    "spheres = SphereMapper(center_column='pt_position', radius_column='radius')\n",
    "anno_layer = AnnotationLayerConfig(name='soma', mapping_rules=spheres)\n",
    "\n",
    "# Make a basic segmentation layer\n",
    "seg_layer = SegmentationLayerConfig(seg_source)\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer, anno_layer])\n",
    "sb.render_state(soma_df, return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching annotations\n",
    "\n",
    "Annotations can be enriched with metadata. There are three main types:\n",
    "1. Descriptions — Each annotation has a free text field.\n",
    "2. Linked Segmentations — Each annotation can have one or more linked segmentation ids. These can be made to automatically load on annotation selection.\n",
    "3. Tags — Neuroglancer states can have discrete tags that are useful for quickly categorizing annotations. Each annotation layer has a list of tags, and each annotation can have any number of tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptions\n",
    "Descriptions need a simple free text field (or None). Any annotation mapper can take a description column with strings that is then displayed with each annotation in Neuroglancer. After running the following code, right click on the annotation layer and you can see that each of the point annotions in the list has an 'e' or 'i' letter underneath it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = PointMapper(point_column='pt_position', description_column='cell_type')\n",
    "\n",
    "anno_layer = AnnotationLayerConfig(name='annos', \n",
    "                                   mapping_rules=points)\n",
    "\n",
    "# Basic segmentation layer\n",
    "seg_layer = SegmentationLayerConfig(seg_source)\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer, anno_layer])\n",
    "sb.render_state(soma_df.head(), return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linked Segmentations\n",
    "\n",
    "An annotation can also be linked to an underlying segmentation, for example a synapse can be linked to its neurons. On the Neuroglancer side, the annotation layer has to know the name of the segmentation layer to use, while the annotation needs to know what root id or ids to look up. To make data-driven annotations with linked segmentations, we both\n",
    "1. Add a linked segmentation column name to the annotation Mapper class that will be one or more column names in the dataframe\n",
    "2. Pass a segmentation layer name to the AnnotationLayerConfig. This defaults to `None`. Note that while the default segmentation layer name is `seg`, no segmentation layer name is set by default. Thus, if you plan to use a segmentation layer that was not given an explicit name, use `seg` as the argument here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = PointMapper('pt_position', linked_segmentation_column='pt_root_id')\n",
    "anno_layer = AnnotationLayerConfig(mapping_rules=points, linked_segmentation_layer='seg')\n",
    "\n",
    "# Basic segmentation layer\n",
    "seg_layer = SegmentationLayerConfig(seg_source)\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer, anno_layer])\n",
    "sb.render_state(soma_df.head(), return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags are categorical labels on annotations. Each annotation layer can have a defined set of up to ten tags (seen under the \"shortcuts\" tab). Each annotation can have any number of tags, toggled on and off with the key command from the shortcut tab. To pre-assign tags to annotations based on the data, you can assign a `tag_column`. For each row in the data, if the element in the tag column is in the annotation layer's tag list, it will assign it to the resulting annotation. Elements of the tag column can also be collections of values, if you want multiple tags assigned. Note that any values that are not in the layer's tag list are ignored.\n",
    "\n",
    "As before, there are two steps:\n",
    "1. If you want to pre-assign tags, add a `tag_column` argument to the annotation Mapper. This isn't needed if you just want to set tags for the layer.\n",
    "2. Pass a list of tags to the AnnotationLayerConfig. The order of the list matters for determining the exact shortcuts that are used for each tag; the first tag has the shortcut `shift-q`, the second tag `shift-w`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = PointMapper('pt_position', tag_column='cell_type')\n",
    "anno_layer = AnnotationLayerConfig(mapping_rules=points, tags=['e'])\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer, anno_layer])\n",
    "sb.render_state(soma_df, return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Setting the View\n",
    "\n",
    "The StateBuilder offers some control over the initial position of the view and how data is visualized, while offering some fairly sensible defaults that look okay in most situations.\n",
    "\n",
    "### Position, layout, and zoom options\n",
    "\n",
    "View options that do not affect individual layers can be set with a dict passed to the `view_kws` argument in StateBuilder, which are passed to `viewer.set_view_options`.\n",
    "* *show_slices* : Boolean, sets if slices are shown in the 3d view. Defaults to False.\n",
    "* *layout* : `xy-3d`/`xz-3d`/`yz-3d` (sections plus 3d pane), `xy`/`yz`/`xz`/`3d` (only one pane), or `4panel` (all panes). Default is `xy-3d`.\n",
    "* *show_axis_lines* : Boolean, determines if the axis lines are shown in the middle of each view.\n",
    "* *show_scale_bar* : Boolean, toggles showing the scale bar.\n",
    "* *orthographic* : Boolean, toggles orthographic view in the 3d pane.\n",
    "* *position* : 3-element vector, determines the centered location.\n",
    "* *zoom_image* : Zoom level for the imagery in units of nm per voxel. Defaults to 8.\n",
    "* *zoom_3d* : Zoom level for the 3d pane. Defaults to 2000. Smaller numbers are more zoomed in.\n",
    "\n",
    "Here's an example of setting some of these rules. Note that only providing default values for some parameters does not override the default values of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_options = {'layout': '4panel',\n",
    "                'show_slices': True,\n",
    "                'zoom_3d': 500,\n",
    "                'position': [71832, 54120, 1089]}\n",
    "\n",
    "seg_layer = SegmentationLayerConfig(seg_source)\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer], view_kws=view_options)\n",
    "sb.render_state(return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data-driven centering\n",
    "\n",
    "It can also be convenient to center the user on an annotation by default.\n",
    "Because this is tied to a specific spatial point column, this option is associated with a particular AnnotationMapper.\n",
    "Data-driven view centering takes precidence over the global view set in `view_kws`.\n",
    "For any of the mappers, setting `set_position` to `True` will center the view on the first annotation in the list.\n",
    "If multiple Mapper objects have `set_position=True`, then the view will follow the end-most one in the end-most annotation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = PointMapper('pt_position', set_position=True)\n",
    "anno_layer = AnnotationLayerConfig(mapping_rules=points)\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer, anno_layer])\n",
    "sb.render_state(soma_df, return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation transparency options\n",
    "\n",
    "Each segmentation layer can control the transparency of selected, unselected, and 3d meshes. As with the global view, these can be passed as keyword arguments to `view_kws` in the SegmentationLayerConfig.\n",
    "* *alpha_selected* : Transparency (0–1) of selected segmentations in the imagery pane. Defaults to 0.3. \n",
    "* *alpha_3d* : Transparency (0–1) of selected meshes in the 3d view. Defaults to 1.\n",
    "* *alpha_unselected* : Transparency (0–1) of unselected segmentations in the imagery pane. Defaults to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_view_options = {'alpha_selected': 0.6,\n",
    "                             'alpha_3d': 0.2}\n",
    "seg_layer = SegmentationLayerConfig(seg_source, fixed_ids=[648518346349539896], view_kws=segmentation_view_options)\n",
    "\n",
    "sb = StateBuilder([img_layer, seg_layer])\n",
    "sb.render_state(return_as='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Chaining StateBuilders to apply multiple rules\n",
    "\n",
    "One limitation of a StateBuilder object is that it applies the one rule to one dataframe. However, one might want to apply multiple rules in succession, for instance viewing both pre- and post-synaptic synapses for a neuron using two dataframes. Because the end result of a StateBuilder is just a Neuroglancer state, we can chain StateBuilders together in a row using the state of the previous as a base state for the next.\n",
    "\n",
    "To make this even easier, there is a ChainedStateBuilder object to help. It takes an ordered collection of StateBuilders and has its own `render_state` method. The key difference of the chained `render_state` is that it takes an ordered collection of dataframes, applying the nth dataframe to the nth StateBuilder.\n",
    "\n",
    "Here, we're going to make a ChainedStateBuilder that will display presynaptic and postsynaptic points for a single neuron, which live in different dataframes. The first statebuilder will handle both setting up the imagery and segmentation as well as rendering the postsynaptic point annotations, while the second one only has to render the presynaptic points, since all the other layers will be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First state builder\n",
    "seg_layer = SegmentationLayerConfig(seg_source, selected_ids_column='post_pt_root_id')\n",
    "\n",
    "postsyn_mapper = LineMapper(point_column_a='pre_pt_position', point_column_b='ctr_pt_position')\n",
    "postsyn_annos = AnnotationLayerConfig('post', color='#00CCCC', mapping_rules=postsyn_mapper)\n",
    "\n",
    "postsyn_sb = StateBuilder(layers=[img_layer, seg_layer, postsyn_annos])\n",
    "\n",
    "#Second state builder\n",
    "presyn_mapper = LineMapper(point_column_a='ctr_pt_position', point_column_b='post_pt_position')\n",
    "presyn_annos = AnnotationLayerConfig('pre', color='#CC1111', mapping_rules=presyn_mapper)\n",
    "\n",
    "presyn_sb = StateBuilder(layers=[presyn_annos])\n",
    "\n",
    "# Chained state builder\n",
    "chained_sb = ChainedStateBuilder([postsyn_sb, presyn_sb])\n",
    "chained_sb.render_state([post_syn_df, pre_syn_df], return_as='html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectomics_analysis",
   "language": "python",
   "name": "connectomics_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
